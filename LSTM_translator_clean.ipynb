{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Notebook Overview </h3>\n",
    "\n",
    "- 1 ) Pre-processing the data\n",
    "- 2 ) Word-based tokenization\n",
    "- 3 ) Creating the embedding matrices\n",
    "- 4 ) Preparing the dataset (batch generator)\n",
    "- 5 ) Create and train the model\n",
    "- 6 ) Predict with the model\n",
    "- Appendix: Translations of sample sentences from both the training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import string\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# df_en_de = pd.read_table('/content/gdrive/MyDrive/deu-eng/deu.txt', names=['eng', 'deu', 'attr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read txt file\n",
    "df_en_de = pd.read_table('deu-eng/deu.txt', names=['eng', 'deu', 'attr'])\n",
    "# drop extraneous column and rename columns\n",
    "df_en_de = df_en_de.drop('attr', axis = 1).rename(columns = {'eng':'english', 'deu':'german'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1 ) Pre-processing the data </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 1.1 ) Pre-process sentences </h4>\n",
    "\n",
    "- all lower-case\n",
    "- remove punctuation\n",
    "- convert umlaut and sharp s (e.g. ä -> ae, ß -> ss)\n",
    "- Add \"START_\" and \"_END\" tokens to target sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all characters\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: x.lower())\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove quotes\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "\n",
    "# Convert umlauts and sharp s:\n",
    "df_en_de['german'] = df_en_de['german'].apply(\n",
    "                            lambda x: x.replace('ü', 'ue').replace('ä', 'ae').replace('ö', 'oe').replace('ß', 'ss')\n",
    "                            )\n",
    "\n",
    "# Create set of all special characters\n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "# Remove all special characters\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "df_en_de['german']=df_en_de['german'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "# Add \"START_\" and \"_END\" tokens to target (German) sentences\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x : 'START_ '+ x + ' _END')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 1.2) Further pre-processing </h4> \n",
    "\n",
    "- Remove ascii errors\n",
    "- Select only the sentences with 10 words or fewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename dataframe\n",
    "pairs = df_en_de\n",
    "\n",
    "# Create new columns showing the number of words per sentence\n",
    "pairs['english_length'] = pairs['english'].apply(lambda x: len(x.split(' ')))\n",
    "pairs['german_length'] = pairs['german'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "# Create new columns with sentences that have ascii symbols removed \n",
    "pairs['english_cleaned'] = pairs['english'].apply(lambda x: x.encode(\"ascii\", \"ignore\"))\n",
    "pairs['english_cleaned'] = pairs['english_cleaned'].apply(lambda x: x.decode())\n",
    "pairs['german_cleaned'] = pairs['german'].apply(lambda x: x.encode(\"ascii\", \"ignore\"))\n",
    "pairs['german_cleaned'] = pairs['german_cleaned'].apply(lambda x: x.decode())\n",
    "\n",
    "# Define max_len\n",
    "max_len = 10\n",
    "\n",
    "# Select only the rows with max_len words or fewer\n",
    "pairs = pairs[pairs['english_length'] <= max_len]\n",
    "pairs = pairs[pairs['german_length'] <= max_len]\n",
    "\n",
    "# Take smaller sample of dataframe (to check code works)\n",
    "pairs = pairs.sample(frac = 0.1, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2) Tokenize the sentences </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-25 15:53:20.815444: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "text_source = pairs['english_cleaned']\n",
    "text_target = pairs['german_cleaned']\n",
    "\n",
    "# create Keras vectorizers \n",
    "Vectorizer_source = TextVectorization()\n",
    "Vectorizer_target = TextVectorization()\n",
    "\n",
    "Vectorizer_source.adapt(text_source)\n",
    "Vectorizer_target.adapt(text_target)\n",
    "\n",
    "# create vocabulary for source (German) and target (English) languages\n",
    "vocab_source = Vectorizer_source.get_vocabulary()\n",
    "vocab_target = Vectorizer_target.get_vocabulary()\n",
    "\n",
    "# convert vocabularies into lists\n",
    "vocab_source = [str(word) for word in vocab_source]\n",
    "vocab_target = [str(word) for word in vocab_target]\n",
    "\n",
    "# remove empty strings\n",
    "vocab_source.remove('')\n",
    "vocab_target.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The embeddings downloaded from spacy don't include our 'START_' and '_END' tokens\n",
    "# Add them to \"vocab_target\"\n",
    "vocab_target.append('START_')\n",
    "vocab_target.append('_END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6446 9992\n"
     ]
    }
   ],
   "source": [
    "# vocab size for source and target languages\n",
    "\n",
    "vocab_len_source = len(vocab_source)\n",
    "vocab_len_target = len (vocab_target)\n",
    "\n",
    "print (vocab_len_source, vocab_len_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3) Create Embedding matrices </h2>\n",
    "\n",
    "- Create embedding matrices for source and target languages\n",
    "- Use pre-trained embeddings from spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Oh/opt/anaconda3/envs/tf/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download en_core_web_lg\n",
    "import en_core_web_lg\n",
    "\n",
    "#!python -m spacy download de_core_news_sm\n",
    "import de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_source = en_core_web_lg.load()\n",
    "nlp_target = de_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the embedding matrix for source vocab\n",
    "\n",
    "# add 1 to size of vocab for zero padding \n",
    "# This is for the Embedding layer later\n",
    "num_tokens_source = vocab_len_source + 1\n",
    "\n",
    "# source language embedding dimensions\n",
    "embedding_dim_source = len(nlp_source('The').vector)\n",
    "\n",
    "# initialise embedding matrix for source language\n",
    "embedding_matrix_source = np.zeros((num_tokens_source, embedding_dim_source))\n",
    "\n",
    "# word-to-index and index-to-word mappings for source language\n",
    "word_idx_source = {}\n",
    "idx_word_source = {}\n",
    "\n",
    "\n",
    "# fill our embedding matrix with pre-trained embeddings from spacy\n",
    "for i, word in enumerate(vocab_source):\n",
    "    # notice we start indexing from 1 (no word is assigned to 0 index)\n",
    "    embedding_matrix_source[i+1] = nlp_source(word).vector      # load vectors into embedding matrix\n",
    "    word_idx_source[word] = int(i+1)                            # word-to-index map\n",
    "    idx_word_source[i+1] = word                                 # index-to-word map\n",
    "\n",
    "\n",
    "# generate the embedding matrix for target vocab\n",
    "\n",
    "# add 1 for zero padding (for Embedding layer)\n",
    "num_tokens_target = vocab_len_target + 1\n",
    "\n",
    "# target language embedding dimensions\n",
    "embedding_dim_target = len(nlp_target('Der').vector)\n",
    "# initialise embedding matrix for target language\n",
    "embedding_matrix_target = np.zeros((num_tokens_target, embedding_dim_target))\n",
    "\n",
    "# word-to-index and index-to-word mappings for target language\n",
    "word_idx_target = {}\n",
    "idx_word_target = {}\n",
    "for i, word in enumerate(vocab_target):\n",
    "    # iterate over all words excluding the final two (\"START_\" and \"_END\")\n",
    "    if i < vocab_len_target - 2 : \n",
    "        embedding_matrix_target[i+1] = nlp_target(word).vector      # load vectors into embedding matrix\n",
    "        word_idx_target[word] = int(i+1)                            # word-to-index map\n",
    "        idx_word_target[i+1] = word                                 # index-to-word map\n",
    "    if word == 'START_':\n",
    "        # assign embedding vector with random values for \"START_\" token \n",
    "        embedding_matrix_target[i+1] = np.random.rand((embedding_dim_target))\n",
    "        word_idx_target[word] = int(i+1)\n",
    "        idx_word_target[i+1] = word\n",
    "    if word == '_END':\n",
    "        # assign embedding vector with random values for \"_END\" token\n",
    "        embedding_matrix_target[i+1] = np.random.rand((embedding_dim_target))\n",
    "        word_idx_target[word] = int(i+1)\n",
    "        idx_word_target[i+1] = word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run time for entire dataset: 3 m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 96\n"
     ]
    }
   ],
   "source": [
    "# Check embedding dimensions\n",
    "print (embedding_dim_source, embedding_dim_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1725 hi\n",
      "46 go\n",
      "1952 market\n",
      "35 kann\n",
      "1725 folgte\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9992, 'geerntet')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity checks -- check that word-to-index and index-to-word mappings are correct\n",
    "print (word_idx_source['hi'], idx_word_source[1725])\n",
    "print (word_idx_source['go'], idx_word_source[46])\n",
    "print (word_idx_source['market'], idx_word_source[1952])\n",
    "print (word_idx_target['kann'], idx_word_target[35])\n",
    "print (word_idx_target['folgte'], idx_word_target[1725])\n",
    "word_idx_target['START_'], idx_word_target[8167]\n",
    "word_idx_target['_END'], idx_word_target[8168]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6447 6447\n",
      "9993 9993\n"
     ]
    }
   ],
   "source": [
    "# sanity checks -- check dimensions of embedding matrices are correct\n",
    "print (embedding_matrix_source.shape[0], num_tokens_source)\n",
    "print(embedding_matrix_target.shape[0], num_tokens_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6447 6446\n",
      "9993 9992\n"
     ]
    }
   ],
   "source": [
    "# sanity checks -- make sure number of tokens = vocab size + 1\n",
    "print (num_tokens_source, len(vocab_source))\n",
    "print (num_tokens_target, len(vocab_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4) Prepare the dataset </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to load CLEANED data\n",
    "X, y = pairs['english_cleaned'], pairs['german_cleaned']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16745,) (16745,)\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        # for every batch j\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            # initialize numpy arrays with zeros          \n",
    "            encoder_input_data = np.zeros((batch_size, max_len), dtype='float32')               \n",
    "            decoder_input_data = np.zeros((batch_size, max_len), dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_len, num_tokens_target), dtype='float32')\n",
    "            \n",
    "            # for every example sentence i\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    # for every time-step t, insert index for encoder input\n",
    "                    encoder_input_data[i, t] = word_idx_source[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    # for every time-step t, insert index for decoder input (excluding final time-step)\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = word_idx_target[word] # decoder input seq\n",
    "                    # create one-hot vector for decoder output, excluding the START_ token\n",
    "                    # offset by one timestep\n",
    "                    if t>0:\n",
    "                        decoder_target_data[i, t - 1, word_idx_target[word]] = 1. \n",
    "                                                            \n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 5) Create the model and train </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_h = 256   # state vector dimension\n",
    "# Notice that embeddings for source and target languages have different lengths\n",
    "emb_dim_source = embedding_matrix_source.shape[1]\n",
    "emb_dim_target = embedding_matrix_target.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All layer objects are global variables. \n",
    "# Their weights are remembered when we call on them in a later model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENCODER ###\n",
    "\n",
    "# Define Input()\n",
    "# Batch-size is automatically \"None\". \n",
    "# We set the time-step dimension as \"None\", which allows time-step dimension of varying length.\n",
    "# This will be useful during the prediction stage, when we will feed one word at a time. \n",
    "encoder_inputs = Input(shape=(None,))                       # (None, None) -- (m, Tx)\n",
    "\n",
    "# Create Embedding layer for encoder, load pre-trained embeddings, freeze weights\n",
    "# Pass input through Embedding layer\n",
    "enc_emb =  Embedding(num_tokens_source, \n",
    "                    emb_dim_source, \n",
    "                    mask_zero = True,\n",
    "                    embeddings_initializer = Constant(embedding_matrix_source),\n",
    "                    trainable = False)(encoder_inputs)                                      # (None, None, emb_dim_source) \n",
    "                                                                                            # -- (m, Tx, input embedding dimensions)\n",
    "\n",
    "# Create LSTM layer\n",
    "# return_state = True: (final_hidden_state, final_hidden_state, final_cell_state)\n",
    "encoder_lstm = LSTM(n_h, return_state=True)\n",
    "\n",
    "# Pass embedding through Encoder LSTM layer\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)                                   # encoder_outputs = (None, n_h)\n",
    "                                                                                            # -- (m, state vector dimensions)\n",
    "# Discard \"encoder_outputs\" and only keep the states.\n",
    "encoder_states = [state_h, state_c]                                                         # state_h = (None, n_h)\n",
    "                                                                                            # -- (m, state vector dimensions)\n",
    "\n",
    "\n",
    "### Decoder ### \n",
    "\n",
    "# Input layer\n",
    "decoder_inputs = Input(shape=(None,))                       # (None, None) -- (m, Ty)\n",
    "\n",
    "# Create Embedding layer for encoder, load pre-trained embeddings, freeze weights                                                     \n",
    "dec_emb_layer = Embedding(num_tokens_target, \n",
    "                        emb_dim_target, \n",
    "                        mask_zero = True,\n",
    "                        embeddings_initializer = Constant(embedding_matrix_target),\n",
    "                        trainable = False)                 \n",
    "\n",
    "# Pass input through Embedding layer\n",
    "dec_emb = dec_emb_layer(decoder_inputs)                                                     # (None, None, emb_dim_target) \n",
    "\n",
    "# Create LSTM layer\n",
    "# return_sequences = True, which means the output will be: \n",
    "# (hidden state for every time-step, hidden state for final time-step, cell state for final time-step)                                                                         \n",
    "decoder_lstm = LSTM(n_h, return_sequences=True, return_state=True)                          # -- (m, Ty, output embedding dimensions)\n",
    "\n",
    "# Pass embedding through Decoder LSTM layer, \n",
    "# using the Encoder's final states as the Decoder's initial states\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
    "                                     initial_state=encoder_states)                          # (None, None, n_h) -- (m, Ty, state vector dimensions)\n",
    "\n",
    "# Create Dense layer with softmax activation\n",
    "decoder_dense = Dense(num_tokens_target, activation='softmax')\n",
    "\n",
    "# Pass Decoder LSTM outputs through Dense layer\n",
    "decoder_outputs = decoder_dense(decoder_outputs)                                            # (None, None, num_tokens_target)\n",
    "                                                                                            # (m, Ty, decoder vocab size + 1)\n",
    "\n",
    "# Define the model\n",
    "# inputs = [encoder_inputs, decoder_inputs]\n",
    "# outputs = decoder_outputs\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)                            # encoder_inputs = (None, None) -- (m, Tx)\n",
    "                                                                                            # decoder_inputs = (None, None) -- (m, Ty)\n",
    "                                                                                            # decoder_outputs = (None, None, decoder vocab size + 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(optimizer= 'Adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total training samples\n",
    "train_samples = len(X_train) \n",
    "# Total validation samples\n",
    "val_samples = len(X_test)    \n",
    "\n",
    "batch_size = 128\n",
    "epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16745,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "130/130 [==============================] - 28s 186ms/step - loss: 3.8525 - acc: 0.2001 - val_loss: 3.4376 - val_acc: 0.2491\n",
      "Epoch 2/40\n",
      "130/130 [==============================] - 24s 183ms/step - loss: 3.1758 - acc: 0.2812 - val_loss: 3.1341 - val_acc: 0.2989\n",
      "Epoch 3/40\n",
      "130/130 [==============================] - 23s 179ms/step - loss: 2.8687 - acc: 0.3179 - val_loss: 2.9406 - val_acc: 0.3271\n",
      "Epoch 4/40\n",
      "130/130 [==============================] - 24s 186ms/step - loss: 2.6372 - acc: 0.3464 - val_loss: 2.8013 - val_acc: 0.3508\n",
      "Epoch 5/40\n",
      "130/130 [==============================] - 25s 196ms/step - loss: 2.4480 - acc: 0.3703 - val_loss: 2.6977 - val_acc: 0.3709\n",
      "Epoch 6/40\n",
      "130/130 [==============================] - 26s 200ms/step - loss: 2.2861 - acc: 0.3907 - val_loss: 2.6144 - val_acc: 0.3868\n",
      "Epoch 7/40\n",
      "130/130 [==============================] - 26s 202ms/step - loss: 2.1409 - acc: 0.4089 - val_loss: 2.5451 - val_acc: 0.3985\n",
      "Epoch 8/40\n",
      "130/130 [==============================] - 27s 205ms/step - loss: 2.0100 - acc: 0.4247 - val_loss: 2.4848 - val_acc: 0.4105\n",
      "Epoch 9/40\n",
      "130/130 [==============================] - 25s 190ms/step - loss: 1.8902 - acc: 0.4413 - val_loss: 2.4351 - val_acc: 0.4206\n",
      "Epoch 10/40\n",
      "130/130 [==============================] - 25s 195ms/step - loss: 1.7779 - acc: 0.4573 - val_loss: 2.4088 - val_acc: 0.4267\n",
      "Epoch 11/40\n",
      "130/130 [==============================] - 26s 201ms/step - loss: 1.6793 - acc: 0.4718 - val_loss: 2.3733 - val_acc: 0.4344\n",
      "Epoch 12/40\n",
      "130/130 [==============================] - 28s 217ms/step - loss: 1.5859 - acc: 0.4902 - val_loss: 2.3465 - val_acc: 0.4414\n",
      "Epoch 13/40\n",
      "130/130 [==============================] - 28s 219ms/step - loss: 1.4940 - acc: 0.5102 - val_loss: 2.3236 - val_acc: 0.4464\n",
      "Epoch 14/40\n",
      "130/130 [==============================] - 26s 199ms/step - loss: 1.4085 - acc: 0.5321 - val_loss: 2.3076 - val_acc: 0.4509\n",
      "Epoch 15/40\n",
      "130/130 [==============================] - 26s 199ms/step - loss: 1.3295 - acc: 0.5551 - val_loss: 2.3230 - val_acc: 0.4496\n",
      "Epoch 16/40\n",
      "130/130 [==============================] - 27s 206ms/step - loss: 1.2625 - acc: 0.5730 - val_loss: 2.2811 - val_acc: 0.4588\n",
      "Epoch 17/40\n",
      "130/130 [==============================] - 28s 214ms/step - loss: 1.1916 - acc: 0.5948 - val_loss: 2.2695 - val_acc: 0.4632\n",
      "Epoch 18/40\n",
      "130/130 [==============================] - 27s 207ms/step - loss: 1.1221 - acc: 0.6170 - val_loss: 2.2588 - val_acc: 0.4675\n",
      "Epoch 19/40\n",
      "130/130 [==============================] - 28s 212ms/step - loss: 1.0578 - acc: 0.6377 - val_loss: 2.2633 - val_acc: 0.4701\n",
      "Epoch 20/40\n",
      "130/130 [==============================] - 26s 200ms/step - loss: 1.0005 - acc: 0.6566 - val_loss: 2.2628 - val_acc: 0.4720\n",
      "Epoch 21/40\n",
      "130/130 [==============================] - 26s 202ms/step - loss: 0.9476 - acc: 0.6744 - val_loss: 2.2603 - val_acc: 0.4738\n",
      "Epoch 22/40\n",
      "130/130 [==============================] - 26s 204ms/step - loss: 0.8970 - acc: 0.6918 - val_loss: 2.2538 - val_acc: 0.4775\n",
      "Epoch 23/40\n",
      "130/130 [==============================] - 27s 207ms/step - loss: 0.8505 - acc: 0.7070 - val_loss: 2.2492 - val_acc: 0.4811\n",
      "Epoch 24/40\n",
      "130/130 [==============================] - 25s 190ms/step - loss: 0.8058 - acc: 0.7231 - val_loss: 2.2530 - val_acc: 0.4832\n",
      "Epoch 25/40\n",
      "130/130 [==============================] - 25s 195ms/step - loss: 0.7632 - acc: 0.7371 - val_loss: 2.2584 - val_acc: 0.4823\n",
      "Epoch 26/40\n",
      "130/130 [==============================] - 27s 211ms/step - loss: 0.7227 - acc: 0.7516 - val_loss: 2.2622 - val_acc: 0.4839\n",
      "Epoch 27/40\n",
      "130/130 [==============================] - 27s 208ms/step - loss: 0.6873 - acc: 0.7636 - val_loss: 2.2728 - val_acc: 0.4834\n",
      "Epoch 28/40\n",
      "130/130 [==============================] - 26s 197ms/step - loss: 0.6558 - acc: 0.7739 - val_loss: 2.2706 - val_acc: 0.4856\n",
      "Epoch 29/40\n",
      "130/130 [==============================] - 27s 211ms/step - loss: 0.6257 - acc: 0.7844 - val_loss: 2.2887 - val_acc: 0.4751\n",
      "Epoch 30/40\n",
      "130/130 [==============================] - 27s 205ms/step - loss: 0.5907 - acc: 0.7971 - val_loss: 2.2927 - val_acc: 0.4789\n",
      "Epoch 31/40\n",
      "130/130 [==============================] - 25s 194ms/step - loss: 0.5535 - acc: 0.8123 - val_loss: 2.2850 - val_acc: 0.4871\n",
      "Epoch 32/40\n",
      "130/130 [==============================] - 24s 187ms/step - loss: 0.5196 - acc: 0.8262 - val_loss: 2.3109 - val_acc: 0.4864\n",
      "Epoch 33/40\n",
      "130/130 [==============================] - 24s 186ms/step - loss: 0.4890 - acc: 0.8376 - val_loss: 2.3167 - val_acc: 0.4865\n",
      "Epoch 34/40\n",
      "130/130 [==============================] - 25s 194ms/step - loss: 0.4629 - acc: 0.8484 - val_loss: 2.3177 - val_acc: 0.4866\n",
      "Epoch 35/40\n",
      "130/130 [==============================] - 25s 189ms/step - loss: 0.4413 - acc: 0.8560 - val_loss: 2.3304 - val_acc: 0.4860\n",
      "Epoch 36/40\n",
      "130/130 [==============================] - 25s 190ms/step - loss: 0.4213 - acc: 0.8630 - val_loss: 2.3363 - val_acc: 0.4864\n",
      "Epoch 37/40\n",
      "130/130 [==============================] - 24s 186ms/step - loss: 0.4014 - acc: 0.8695 - val_loss: 2.3546 - val_acc: 0.4803\n",
      "Epoch 38/40\n",
      "130/130 [==============================] - 24s 187ms/step - loss: 0.3793 - acc: 0.8771 - val_loss: 2.3546 - val_acc: 0.4895\n",
      "Epoch 39/40\n",
      "130/130 [==============================] - 25s 189ms/step - loss: 0.3547 - acc: 0.8879 - val_loss: 2.3940 - val_acc: 0.4893\n",
      "Epoch 40/40\n",
      "130/130 [==============================] - 25s 195ms/step - loss: 0.3305 - acc: 0.8974 - val_loss: 2.3774 - val_acc: 0.4919\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(generate_batch(), \n",
    "                    steps_per_epoch=train_samples//batch_size, \n",
    "                    epochs = epochs,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples // batch_size, \n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 6) Inference with the model </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At this stage, the LSTMs in our Encoder and Decoder have been trained (their weights/variables have been optimized)\n",
    "- Inference / prediction now takes place in two steps. \n",
    "- 1 ) Pass source sequence through Encoder's LSTMs to get the final hidden and cell state vectors\n",
    "- 2 ) We will predict the target sequence one time-step (one LSTM cell) at a time. All LSTM cells share the same weights.\n",
    "- 2.0) Pass in the Encoder's final hidden and cell states as the Decoder's initial hidden and cell states\n",
    "- 2.1) Pass in the predicted output from previous Decoder LSTM cell as the input. \n",
    "- 2.2) Pass in the hidden and cell states from previous Decocder LSTM cell as the initial states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENCODER ###\n",
    "\n",
    "# Create an encoder_model, using the same \n",
    "# \"encoder_inputs\" and \"encoder_states\" that we trained above (global variables)\n",
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)                   \n",
    "                                                                        # encoder_inputs -- (None, None) -- (m, Tx)\n",
    "                                                                        # encoder_states = [state_h, state_c] -- [(None, n_h), (None,n_h)] \n",
    "\n",
    "### DECODER ###\n",
    "\n",
    "# Define Inputs for Decoder's hidden and cell states\n",
    "decoder_state_input_h = Input(shape=(n_h,))                      # (None, n_h) -- (m, state vector dims)   \n",
    "decoder_state_input_c = Input(shape=(n_h,))                      # (None, n_h) -- (m, state vector dims)\n",
    "decoder_state_input = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# Pass Decoder input through Embedding layer \n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)                                # (None, None, emb_dim_target) -- (m, Ty, embedding dims)\n",
    "\n",
    "# To predict the next word in the sequence, \n",
    "# pass the Decoder states from the previous time-step as the initial states\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(\n",
    "                                            inputs = dec_emb2, \n",
    "                                            initial_state = decoder_state_input\n",
    "                                            )                           \n",
    "                                            \n",
    "                                                                        # decoder_outputs2 = (None, None, n_h) -- (m, Ty, state vector dims)\n",
    "                                                                        # state_h2 = (None, n_h) -- (m, state vector dims)\n",
    "                                                                        # state_c2 = (None, n_h) -- (m, state vector dims)\n",
    "\n",
    "                                                                        \n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# Pass Decoder outputs through Dense layer with softmax activation \n",
    "# to get probability distribution over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)                      # (None, None, num_decoder_tokens) -- (m, Ty, target vocab size + 1)\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    inputs = [decoder_inputs] + decoder_state_input,\n",
    "    outputs = [decoder_outputs2] + decoder_states2)                     # decoder_inputs = (None, None) -- (m, Ty)\n",
    "                                                                        # decoder_state_input = [(m, state vector dims), (m, state vector dims)]\n",
    "                                                                        # decoder_outputs2 = (m, Ty, target vocab size + 1)\n",
    "                                                                        # decoder_states2 = [(m, state vector dims), (m, state vector dims)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Functions for making inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_seq(sentence):\n",
    "    \"\"\"\n",
    "    Converts sentence (string) into sequence of integers\n",
    "\n",
    "    Arguments\n",
    "    sentence -- string\n",
    "    Returns\n",
    "    encoder_input_data -- (1, max_len)\n",
    "    \"\"\"\n",
    "    # Initialise numpy array with zeros\n",
    "    encoder_input_data = np.zeros((1, max_len))\n",
    "\n",
    "    # Convert into list of words\n",
    "    sentence = sentence.lower().split()\n",
    "    \n",
    "    # Place every j'th word in \"sentence\" into j'th position of \"encoder_input_data\"\n",
    "    for j, word in enumerate(sentence):\n",
    "        encoder_input_data[0,j] = word_idx_source[word]\n",
    "    return encoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "\n",
    "    \"\"\" \n",
    "    Translates source sequence into target sentence. \n",
    "    This function predicts one sentence at a time. \n",
    "\n",
    "    Arguments\n",
    "    input_seq       -- string\n",
    "    \n",
    "    Returns\n",
    "    decoded_sentence -- string\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Pass source sequence through the encoder_model to get the final state and cell states.\n",
    "    states_value = encoder_model.predict(input_seq)                     # states_value = [state_h, state_c]\n",
    "                                                                        # [(None, n_h), (None,n_h)] \n",
    "                                                                        # -- [(m, state vector dims), (m, state vector dims)]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first position of \"target_seq\" with the index for the \"START_\" token.\n",
    "    target_seq[0, 0] = word_idx_target['START_']\n",
    "\n",
    "    # Now we will predict the target sequence one time-step at a time\n",
    "    # For the Decoder's initial hidden and cell states, use the Encoder's final hidden and cell states.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    # Note: target_seq will always be a single integer\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)        \n",
    "                                                                                        \n",
    "                                                                                        # output_tokens = softmax output = (m, Ty, num_tokens_target)\n",
    "                                                                                       \n",
    "        # Find index with max. probability\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])                # Note: we'll always be predicting one time-step at a time, so Ty = 1.\n",
    "                                                                                # But set index 1 of output_tokens as \"-1\" for generality.\n",
    "        # Map index to word\n",
    "        sampled_word = idx_word_target[sampled_token_index]\n",
    "\n",
    "        # Append sampled word to \"decoded_sentence\"\n",
    "        decoded_sentence += ' '+ sampled_word\n",
    "\n",
    "        # Exit condition: either hit max_len or sampled_word = \"_END\"\n",
    "        if (sampled_word == '_END' or len(decoded_sentence.split(' ')) > max_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update Decoder states\n",
    "        states_value = [h, c]\n",
    "        \n",
    "    return decoded_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Translate a sample of input sentences from training data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: why didnt tom visit boston \n",
      " Predicted Translation:  warum ist tom boston nicht besucht _END \n",
      " True Translation: START_ warum hat tom boston nicht besucht _END \n",
      "\n",
      "Source: please dont waste electricity \n",
      " Predicted Translation:  bitte keinen strom verschwenden _END \n",
      " True Translation: START_ bitte keinen strom verschwenden _END \n",
      "\n",
      "Source: tom insulted the waiter \n",
      " Predicted Translation:  tom beleidigte sich die schulter _END \n",
      " True Translation: START_ tom beleidigte den kellner _END \n",
      "\n",
      "Source: theres nothing to worry about \n",
      " Predicted Translation:  es ist nur etwas anderes zu sagen _END \n",
      " True Translation: START_ es gibt keinen grund zur aufregung _END \n",
      "\n",
      "Source: they appointed him manager \n",
      " Predicted Translation:  sie ernannten ihn zum buergermeister _END \n",
      " True Translation: START_ sie ernannten ihn zum manager _END \n",
      "\n",
      "Source: one more bottle of wine please \n",
      " Predicted Translation:  bitte ist eine flasche milch _END \n",
      " True Translation: START_ noch eine flasche wein bitte _END \n",
      "\n",
      "Source: do you know what tom said about it \n",
      " Predicted Translation:  weisst du was tom gesagt hat _END \n",
      " True Translation: START_ weisst du was tom darueber gesagt hat _END \n",
      "\n",
      "Source: tom is always making me angry \n",
      " Predicted Translation:  tom ist immer wuetend _END \n",
      " True Translation: START_ tom macht mich immer wuetend _END \n",
      "\n",
      "Source: i want to speak with tom please \n",
      " Predicted Translation:  ich moechte mit tom sprechen _END \n",
      " True Translation: START_ ich moechte bitte mit tom sprechen _END \n",
      "\n",
      "Source: arent you pushing it too far \n",
      " Predicted Translation:  treibst du es nicht ganz zu sein _END \n",
      " True Translation: START_ treibst du es nicht ein bisschen zu weit _END \n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "\n",
    "# Pick first N sentences from training data\n",
    "sentences = list(X_train.iloc[:N].values)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    seq = sentence_to_seq(sentence)\n",
    "    translation = decode_sequence(seq)\n",
    "    true_translation = y_train.iloc[i]\n",
    "    print(f'Source: {sentence} \\n Predicted Translation: {translation} \\n True Translation: {true_translation} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Translate a sample of input sentences from test data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: i could never give up meat \n",
      " Predicted Translation:  ich koennte nie aufgeben sollen _END \n",
      " True Translation: START_ ich koennte niemals ohne fleisch auskommen _END \n",
      "\n",
      "Source: he often plays guitar \n",
      " Predicted Translation:  er spielt oft klavier _END \n",
      " True Translation: START_ er spielt oft gitarre _END \n",
      "\n",
      "Source: tom and mary didnt have a choice \n",
      " Predicted Translation:  tom und maria haben nicht mehr _END \n",
      " True Translation: START_ tom und maria hatten keine wahl _END \n",
      "\n",
      "Source: the water is nice and cool \n",
      " Predicted Translation:  die akustik ist sehr schoen _END \n",
      " True Translation: START_ das wasser ist angenehm kuehl _END \n",
      "\n",
      "Source: which browser are you using \n",
      " Predicted Translation:  welche farbe ist du _END \n",
      " True Translation: START_ welchen netzgucker gebrauchst du _END \n",
      "\n",
      "Source: do you have one that is a little smaller \n",
      " Predicted Translation:  hast du etwas eine ahnung dass es _END \n",
      " True Translation: START_ hast du eins welches kleiner ist _END \n",
      "\n",
      "Source: i forgot to close the door \n",
      " Predicted Translation:  ich habe das fenster zugemacht zu sein _END \n",
      " True Translation: START_ ich vergass die tuer zu schliessen _END \n",
      "\n",
      "Source: im divorced \n",
      " Predicted Translation:  ich bin kanadier _END \n",
      " True Translation: START_ ich bin geschieden _END \n",
      "\n",
      "Source: clean up the kitchen \n",
      " Predicted Translation:  bring den raum in ordnung _END \n",
      " True Translation: START_ mach die kueche sauber _END \n",
      "\n",
      "Source: tom likes writing \n",
      " Predicted Translation:  tom mag gerne _END \n",
      " True Translation: START_ tom schreibt gerne _END \n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "\n",
    "# Pick first N sentences from test data\n",
    "sentences = list(X_test.iloc[:N].values)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    seq = sentence_to_seq(sentence)\n",
    "    translation = decode_sequence(seq)\n",
    "    true_translation = y_test.iloc[i]\n",
    "    print(f'Source: {sentence} \\n Predicted Translation: {translation} \\n True Translation: {true_translation} \\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a30d934768a106a0bdaa0b54b1b0ce58ac936216b32c4f047caaf50b54e34c32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
