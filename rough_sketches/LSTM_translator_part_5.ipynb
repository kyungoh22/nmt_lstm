{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I was able to get a working model for LSTM translation in part 4\n",
    "- But so far, I have not been using pre-trained embeddings\n",
    "- In this notebook, I build an LSTM translation model using pre-trained weights from spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import string\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# df_en_de = pd.read_table('/content/gdrive/MyDrive/deu-eng/deu.txt', names=['eng', 'deu', 'attr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read txt file\n",
    "df_en_de = pd.read_table('deu-eng/deu.txt', names=['eng', 'deu', 'attr'])\n",
    "# drop extraneous column and rename columns\n",
    "df_en_de = df_en_de.drop('attr', axis = 1).rename(columns = {'eng':'english', 'deu':'german'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pre-process the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all characters\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: x.lower())\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove quotes\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "\n",
    "# Convert umlauts and sharp s:\n",
    "df_en_de['german'] = df_en_de['german'].apply(\n",
    "                            lambda x: x.replace('ü', 'ue').replace('ä', 'ae').replace('ö', 'oe').replace('ß', 'ss')\n",
    "                            )\n",
    "\n",
    "# Create set of all special characters\n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "# Remove all special characters\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "df_en_de['german']=df_en_de['german'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "# Add \"START_\" and \"_END\" tokens to target (German) sentences\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x : 'START_ '+ x + ' _END')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Further pre-processing + only select sentences with 10 words or fewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename dataframe\n",
    "pairs = df_en_de\n",
    "\n",
    "# Create new columns showing the number of words per sentence\n",
    "pairs['english_length'] = pairs['english'].apply(lambda x: len(x.split(' ')))\n",
    "pairs['german_length'] = pairs['german'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "# Create new columns with sentences that have ascii symbols removed \n",
    "pairs['english_cleaned'] = pairs['english'].apply(lambda x: x.encode(\"ascii\", \"ignore\"))\n",
    "pairs['english_cleaned'] = pairs['english_cleaned'].apply(lambda x: x.decode())\n",
    "pairs['german_cleaned'] = pairs['german'].apply(lambda x: x.encode(\"ascii\", \"ignore\"))\n",
    "pairs['german_cleaned'] = pairs['german_cleaned'].apply(lambda x: x.decode())\n",
    "\n",
    "# Define max_len\n",
    "max_len = 10\n",
    "\n",
    "# Select only the rows with max_len words or fewer\n",
    "pairs = pairs[pairs['english_length'] <= max_len]\n",
    "pairs = pairs[pairs['german_length'] <= max_len]\n",
    "\n",
    "# Take smaller sample of dataframe (to check code works)\n",
    "pairs = pairs.sample(frac = 0.1, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenize sentences (word-based) using spacy modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Oh/opt/anaconda3/envs/tf/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download en_core_web_lg\n",
    "import en_core_web_lg\n",
    "\n",
    "#!python -m spacy download de_core_news_sm\n",
    "import de_core_news_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-25 11:45:06.655098: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "text_source = pairs['english_cleaned']\n",
    "text_target = pairs['german_cleaned']\n",
    "\n",
    "nlp_source = en_core_web_lg.load()\n",
    "nlp_target = de_core_news_sm.load()\n",
    "\n",
    "# create Keras vectorizers \n",
    "Vectorizer_source = TextVectorization()\n",
    "Vectorizer_target = TextVectorization()\n",
    "\n",
    "Vectorizer_source.adapt(text_source)\n",
    "Vectorizer_target.adapt(text_target)\n",
    "\n",
    "# create vocabulary for source (German) and target (English) languages\n",
    "vocab_source = Vectorizer_source.get_vocabulary()\n",
    "vocab_target = Vectorizer_target.get_vocabulary()\n",
    "\n",
    "# convert vocabularies into lists\n",
    "vocab_source = [str(word) for word in vocab_source]\n",
    "vocab_target = [str(word) for word in vocab_target]\n",
    "\n",
    "# remove empty strings\n",
    "vocab_source.remove('')\n",
    "vocab_target.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The embeddings downloaded from spacy don't include our 'START_' and '_END' tokens\n",
    "# Add them to \"vocab_target\"\n",
    "vocab_target.append('START_')\n",
    "vocab_target.append('_END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6446 9992\n"
     ]
    }
   ],
   "source": [
    "# vocab size for source and target languages\n",
    "\n",
    "vocab_len_source = len(vocab_source)\n",
    "vocab_len_target = len (vocab_target)\n",
    "\n",
    "print (vocab_len_source, vocab_len_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create embedding matrices for source and target languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the embedding matrix for source vocab\n",
    "\n",
    "# add 1 to size of vocab for zero padding \n",
    "# This is for the Embedding layer later\n",
    "num_tokens_source = vocab_len_source + 1\n",
    "\n",
    "# source language embedding dimensions\n",
    "embedding_dim_source = len(nlp_source('The').vector)\n",
    "\n",
    "# initialise embedding matrix for source language\n",
    "embedding_matrix_source = np.zeros((num_tokens_source, embedding_dim_source))\n",
    "\n",
    "# word-to-index and index-to-word mappings for source language\n",
    "word_idx_source = {}\n",
    "idx_word_source = {}\n",
    "\n",
    "\n",
    "# fill our embedding matrix with pre-trained embeddings from spacy\n",
    "for i, word in enumerate(vocab_source):\n",
    "    # notice we start indexing from 1 (no word is assigned to 0 index)\n",
    "    embedding_matrix_source[i+1] = nlp_source(word).vector      # load vectors into embedding matrix\n",
    "    word_idx_source[word] = int(i+1)                            # word-to-index map\n",
    "    idx_word_source[i+1] = word                                 # index-to-word map\n",
    "\n",
    "\n",
    "# generate the embedding matrix for target vocab\n",
    "\n",
    "# add 1 for zero padding (for Embedding layer)\n",
    "num_tokens_target = vocab_len_target + 1\n",
    "\n",
    "# target language embedding dimensions\n",
    "embedding_dim_target = len(nlp_target('Der').vector)\n",
    "# initialise embedding matrix for target language\n",
    "embedding_matrix_target = np.zeros((num_tokens_target, embedding_dim_target))\n",
    "\n",
    "# word-to-index and index-to-word mappings for target language\n",
    "word_idx_target = {}\n",
    "idx_word_target = {}\n",
    "for i, word in enumerate(vocab_target):\n",
    "    # iterate over all words excluding the final two (\"START_\" and \"_END\")\n",
    "    if i < vocab_len_target - 2 : \n",
    "        embedding_matrix_target[i+1] = nlp_target(word).vector      # load vectors into embedding matrix\n",
    "        word_idx_target[word] = int(i+1)                            # word-to-index map\n",
    "        idx_word_target[i+1] = word                                 # index-to-word map\n",
    "    if word == 'START_':\n",
    "        # assign embedding vector with random values for \"START_\" token \n",
    "        embedding_matrix_target[i+1] = np.random.rand((embedding_dim_target))\n",
    "        word_idx_target[word] = int(i+1)\n",
    "        idx_word_target[i+1] = word\n",
    "    if word == '_END':\n",
    "        # assign embedding vector with random values for \"_END\" token\n",
    "        embedding_matrix_target[i+1] = np.random.rand((embedding_dim_target))\n",
    "        word_idx_target[word] = int(i+1)\n",
    "        idx_word_target[i+1] = word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run time for entire dataset: 3 m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1725 hi\n",
      "46 go\n",
      "1952 market\n",
      "35 kann\n",
      "1725 folgte\n"
     ]
    }
   ],
   "source": [
    "# sanity checks -- check that word-to-index and index-to-word mappings are correct\n",
    "print (word_idx_source['hi'], idx_word_source[1725])\n",
    "print (word_idx_source['go'], idx_word_source[46])\n",
    "print (word_idx_source['market'], idx_word_source[1952])\n",
    "print (word_idx_target['kann'], idx_word_target[35])\n",
    "print (word_idx_target['folgte'], idx_word_target[1725])\n",
    "# word_idx_target['START_'], idx_word_target[8167]\n",
    "#word_idx_target['_END'], idx_word_target[8168]\n",
    "#word_idx_target.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6447 6447\n",
      "9993 9993\n"
     ]
    }
   ],
   "source": [
    "# sanity checks -- check dimensions of embedding matrices are correct\n",
    "print (embedding_matrix_source.shape[0], num_tokens_source)\n",
    "print(embedding_matrix_target.shape[0], num_tokens_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6447 6446\n",
      "9993 9992\n"
     ]
    }
   ],
   "source": [
    "# Note on dimensions\n",
    "# embedding_matrix_source.shape = (num_tokens_source, embedding_dim_source)\n",
    "# embedding_matrix_target.shape = (num_tokens_target, embedding_dim_target)\n",
    "# num_tokens_source = len(vocab_source) + 1\n",
    "# num_tokens_target = len (vocab_target) + 1\n",
    "\n",
    "# sanity checks -- \n",
    "print (num_tokens_source, len(vocab_source))\n",
    "print (num_tokens_target, len(vocab_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to load CLEANED data\n",
    "X, y = pairs['english_cleaned'], pairs['german_cleaned']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16745,) (16745,)\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        # for every batch j\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            # initialize numpy arrays with zeros          \n",
    "            encoder_input_data = np.zeros((batch_size, max_len), dtype='float32')               \n",
    "            decoder_input_data = np.zeros((batch_size, max_len), dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_len, num_tokens_target), dtype='float32')\n",
    "            \n",
    "            # for every example sentence i\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    # for every time-step t, insert index for encoder input\n",
    "                    encoder_input_data[i, t] = word_idx_source[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    # for every time-step t, insert index for decoder input (excluding final time-step)\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = word_idx_target[word] # decoder input seq\n",
    "                    # create one-hot vector for decoder output, excluding the START_ token\n",
    "                    # offset by one timestep\n",
    "                    if t>0:\n",
    "                        decoder_target_data[i, t - 1, word_idx_target[word]] = 1. \n",
    "                                                            \n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Model for training </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_h = 256   # state vector dimension\n",
    "# Notice that embeddings for source and target languages have different lengths\n",
    "emb_dim_source = embedding_matrix_source.shape[1]\n",
    "emb_dim_target = embedding_matrix_target.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All layer objects are global variables. \n",
    "# Their weights are remembered when we call on them in a later model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENCODER ###\n",
    "\n",
    "# Define Input()\n",
    "# Batch-size is automatically \"None\". \n",
    "# We set the time-step dimension as \"None\", which allows time-step dimension of varying length.\n",
    "# This will be useful during the prediction stage, when we will feed one word at a time. \n",
    "encoder_inputs = Input(shape=(None,))                       # (None, None) -- (m, Tx)\n",
    "\n",
    "# Create Embedding layer for encoder, load pre-trained embeddings, freeze weights\n",
    "# Pass input through Embedding layer\n",
    "enc_emb =  Embedding(num_tokens_source, \n",
    "                    emb_dim_source, \n",
    "                    mask_zero = True,\n",
    "                    embeddings_initializer = Constant(embedding_matrix_source),\n",
    "                    trainable = False)(encoder_inputs)                                      # (None, None, emb_dim_source) \n",
    "                                                                                            # -- (m, Tx, input embedding dimensions)\n",
    "\n",
    "# Create LSTM layer\n",
    "# return_state = True: (final_hidden_state, final_hidden_state, final_cell_state)\n",
    "encoder_lstm = LSTM(n_h, return_state=True)\n",
    "\n",
    "# Pass embedding through Encoder LSTM layer\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)                                   # encoder_outputs = (None, n_h)\n",
    "                                                                                            # -- (m, state vector dimensions)\n",
    "# Discard \"encoder_outputs\" and only keep the states.\n",
    "encoder_states = [state_h, state_c]                                                         # state_h = (None, n_h)\n",
    "                                                                                            # -- (m, state vector dimensions)\n",
    "\n",
    "\n",
    "### Decoder ### \n",
    "\n",
    "# Input layer\n",
    "decoder_inputs = Input(shape=(None,))                       # (None, None) -- (m, Ty)\n",
    "\n",
    "# Create Embedding layer for encoder, load pre-trained embeddings, freeze weights                                                     \n",
    "dec_emb_layer = Embedding(num_tokens_target, \n",
    "                        emb_dim_target, \n",
    "                        mask_zero = True,\n",
    "                        embeddings_initializer = Constant(embedding_matrix_target),\n",
    "                        trainable = False)                 \n",
    "\n",
    "# Pass input through Embedding layer\n",
    "dec_emb = dec_emb_layer(decoder_inputs)                                                     # (None, None, emb_dim_target) \n",
    "\n",
    "# Create LSTM layer\n",
    "# return_sequences = True, which means the output will be: \n",
    "# (hidden state for every time-step, hidden state for final time-step, cell state for final time-step)                                                                         \n",
    "decoder_lstm = LSTM(n_h, return_sequences=True, return_state=True)                          # -- (m, Ty, output embedding dimensions)\n",
    "\n",
    "# Pass embedding through Decoder LSTM layer, \n",
    "# using the Encoder's final states as the Decoder's initial states\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
    "                                     initial_state=encoder_states)                          # (None, None, n_h) -- (m, Ty, state vector dimensions)\n",
    "\n",
    "# Create Dense layer with softmax activation\n",
    "decoder_dense = Dense(num_tokens_target, activation='softmax')\n",
    "\n",
    "# Pass Decoder LSTM outputs through Dense layer\n",
    "decoder_outputs = decoder_dense(decoder_outputs)                                            # (None, None, num_tokens_target)\n",
    "                                                                                            # (m, Ty, decoder vocab size + 1)\n",
    "\n",
    "\n",
    "\n",
    "# Define the model\n",
    "# inputs = [encoder_inputs, decoder_inputs]\n",
    "# outputs = decoder_outputs\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)                            # encoder_inputs = (None, None) -- (m, Tx)\n",
    "                                                                                            # decoder_inputs = (None, None) -- (m, Ty)\n",
    "                                                                                            # decoder_outputs = (None, None, decoder vocab size + 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(optimizer= 'Adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total training samples\n",
    "train_samples = len(X_train) \n",
    "# Total validation samples\n",
    "val_samples = len(X_test)    \n",
    "\n",
    "batch_size = 128\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16745,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "130/130 [==============================] - 25s 189ms/step - loss: 0.5529 - acc: 0.8129 - val_loss: 2.2760 - val_acc: 0.4862\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 25s 191ms/step - loss: 0.5113 - acc: 0.8293 - val_loss: 2.2854 - val_acc: 0.4897\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 24s 184ms/step - loss: 0.4847 - acc: 0.8404 - val_loss: 2.2958 - val_acc: 0.4893\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 24s 184ms/step - loss: 0.4630 - acc: 0.8478 - val_loss: 2.3061 - val_acc: 0.4864\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 25s 189ms/step - loss: 0.4465 - acc: 0.8531 - val_loss: 2.3202 - val_acc: 0.4838\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 25s 193ms/step - loss: 0.4269 - acc: 0.8582 - val_loss: 2.3290 - val_acc: 0.4876\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 24s 185ms/step - loss: 0.3968 - acc: 0.8716 - val_loss: 2.3414 - val_acc: 0.4898\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 25s 190ms/step - loss: 0.3675 - acc: 0.8837 - val_loss: 2.3437 - val_acc: 0.4891\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 26s 199ms/step - loss: 0.3430 - acc: 0.8948 - val_loss: 2.3617 - val_acc: 0.4895\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 27s 207ms/step - loss: 0.3201 - acc: 0.9041 - val_loss: 2.3865 - val_acc: 0.4897\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(generate_batch(), \n",
    "                    steps_per_epoch=train_samples//batch_size, \n",
    "                    epochs = epochs,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples // batch_size, \n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Predicting with the model </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At this stage, the LSTMs in our Encoder and Decoder have been trained (their weights/variables have been optimized)\n",
    "- Inference / prediction now takes place in two steps. \n",
    "- 1 ) Pass source sequence through Encoder's LSTMs to get the final hidden and cell state vectors\n",
    "- 2 ) We will predict the target sequence one time-step (one LSTM cell) at a time. All LSTM cells share the same weights.\n",
    "- 2.0) Pass in the Encoder's final hidden and cell states as the Decoder's initial hidden and cell states\n",
    "- 2.1) Pass in the predicted output from previous Decoder LSTM cell as the input. \n",
    "- 2.2) Pass in the hidden and cell states from previous Decocder LSTM cell as the initial states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENCODER ###\n",
    "\n",
    "# Create an encoder_model, using the same \n",
    "# \"encoder_inputs\" and \"encoder_states\" that we trained above (global variables)\n",
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)                   \n",
    "                                                                        # encoder_inputs -- (None, None) -- (m, Tx)\n",
    "                                                                        # encoder_states = [state_h, state_c] -- [(None, n_h), (None,n_h)] \n",
    "\n",
    "### DECODER ###\n",
    "\n",
    "# Define Inputs for Decoder's hidden and cell states\n",
    "decoder_state_input_h = Input(shape=(n_h,))                      # (None, n_h) -- (m, state vector dims)   \n",
    "decoder_state_input_c = Input(shape=(n_h,))                      # (None, n_h) -- (m, state vector dims)\n",
    "decoder_state_input = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# Pass Decoder input through Embedding layer \n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)                                # (None, None, emb_dim_target) -- (m, Ty, embedding dims)\n",
    "\n",
    "# To predict the next word in the sequence, \n",
    "# pass the Decoder states from the previous time-step as the initial states\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(\n",
    "                                            inputs = dec_emb2, \n",
    "                                            initial_state = decoder_state_input\n",
    "                                            )                           \n",
    "                                            \n",
    "                                                                        # decoder_outputs2 = (None, None, n_h) -- (m, Ty, state vector dims)\n",
    "                                                                        # state_h2 = (None, n_h) -- (m, state vector dims)\n",
    "                                                                        # state_c2 = (None, n_h) -- (m, state vector dims)\n",
    "\n",
    "                                                                        \n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# Pass Decoder outputs through Dense layer with softmax activation \n",
    "# to get probability distribution over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)                      # (None, None, num_decoder_tokens) -- (m, Ty, target vocab size + 1)\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    inputs = [decoder_inputs] + decoder_state_input,\n",
    "    outputs = [decoder_outputs2] + decoder_states2)                     # decoder_inputs = (None, None) -- (m, Ty)\n",
    "                                                                        # decoder_state_input = [(m, state vector dims), (m, state vector dims)]\n",
    "                                                                        # decoder_outputs2 = (m, Ty, target vocab size + 1)\n",
    "                                                                        # decoder_states2 = [(m, state vector dims), (m, state vector dims)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Functions for making inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_seq(sentence):\n",
    "    \"\"\"\n",
    "    Converts sentence (string) into sequence of integers\n",
    "\n",
    "    Arguments\n",
    "    sentence -- string\n",
    "    Returns\n",
    "    encoder_input_data -- (1, max_len)\n",
    "    \"\"\"\n",
    "    # Initialise numpy array with zeros\n",
    "    encoder_input_data = np.zeros((1, max_len))\n",
    "\n",
    "    # Convert into list of words\n",
    "    sentence = sentence.lower().split()\n",
    "    \n",
    "    # Place every j'th word in \"sentence\" into j'th position of \"encoder_input_data\"\n",
    "    for j, word in enumerate(sentence):\n",
    "        encoder_input_data[0,j] = word_idx_source[word]\n",
    "    return encoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "\n",
    "    \"\"\" \n",
    "    Translates source sequence into target sentence. \n",
    "    This function predicts one sentence at a time. \n",
    "\n",
    "    Arguments\n",
    "    input_seq       -- string\n",
    "    \n",
    "    Returns\n",
    "    decoded_sentence -- string\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Pass source sequence through the encoder_model to get the final state and cell states.\n",
    "    states_value = encoder_model.predict(input_seq)                     # states_value = [state_h, state_c]\n",
    "                                                                        # [(None, n_h), (None,n_h)] \n",
    "                                                                        # -- [(m, state vector dims), (m, state vector dims)]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first position of \"target_seq\" with the index for the \"START_\" token.\n",
    "    target_seq[0, 0] = word_idx_target['START_']\n",
    "\n",
    "    # Now we will predict the target sequence one time-step at a time\n",
    "    # For the Decoder's initial hidden and cell states, use the Encoder's final hidden and cell states.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    # Note: target_seq will always be a single integer\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)        \n",
    "                                                                                        \n",
    "                                                                                        # output_tokens = softmax output = (m, Ty, num_tokens_target)\n",
    "                                                                                       \n",
    "        # Find index with max. probability\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])                # Note: we'll always be predicting one time-step at a time, so Ty = 1.\n",
    "                                                                                # But set index 1 of output_tokens as \"-1\" for generality.\n",
    "        # Map index to word\n",
    "        sampled_word = idx_word_target[sampled_token_index]\n",
    "\n",
    "        # Append sampled word to \"decoded_sentence\"\n",
    "        decoded_sentence += ' '+ sampled_word\n",
    "\n",
    "        # Exit condition: either hit max_len or sampled_word = \"_END\"\n",
    "        if (sampled_word == '_END' or len(decoded_sentence.split(' ')) > max_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update Decoder states\n",
    "        states_value = [h, c]\n",
    "        \n",
    "    return decoded_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Translate a sample of input sentences from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: why didnt tom visit boston \n",
      " Predicted Translation:  warum hat tom nicht nach boston _END \n",
      " True Translation: START_ warum hat tom boston nicht besucht _END \n",
      "\n",
      "Source: please dont waste electricity \n",
      " Predicted Translation:  bitte keine sorge das wasser _END \n",
      " True Translation: START_ bitte keinen strom verschwenden _END \n",
      "\n",
      "Source: tom insulted the waiter \n",
      " Predicted Translation:  tom beleidigte den kellner _END \n",
      " True Translation: START_ tom beleidigte den kellner _END \n",
      "\n",
      "Source: theres nothing to worry about \n",
      " Predicted Translation:  es gibt keine angst von euch _END \n",
      " True Translation: START_ es gibt keinen grund zur aufregung _END \n",
      "\n",
      "Source: they appointed him manager \n",
      " Predicted Translation:  sie ernannten ihn zum manager _END \n",
      " True Translation: START_ sie ernannten ihn zum manager _END \n",
      "\n",
      "Source: one more bottle of wine please \n",
      " Predicted Translation:  noch eine flasche wein bitte _END \n",
      " True Translation: START_ noch eine flasche wein bitte _END \n",
      "\n",
      "Source: do you know what tom said about it \n",
      " Predicted Translation:  weisst du was tom darueber gesagt hat _END \n",
      " True Translation: START_ weisst du was tom darueber gesagt hat _END \n",
      "\n",
      "Source: tom is always making me angry \n",
      " Predicted Translation:  tom macht mich immer wuetend _END \n",
      " True Translation: START_ tom macht mich immer wuetend _END \n",
      "\n",
      "Source: i want to speak with tom please \n",
      " Predicted Translation:  ich moechte mit tom sprechen _END \n",
      " True Translation: START_ ich moechte bitte mit tom sprechen _END \n",
      "\n",
      "Source: arent you pushing it too far \n",
      " Predicted Translation:  treibst du es nicht so weit _END \n",
      " True Translation: START_ treibst du es nicht ein bisschen zu weit _END \n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "\n",
    "# Pick first N sentences from training data\n",
    "sentences = list(X_train.iloc[:N].values)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    seq = sentence_to_seq(sentence)\n",
    "    translation = decode_sequence(seq)\n",
    "    true_translation = y_train.iloc[i]\n",
    "    print(f'Source: {sentence} \\n Predicted Translation: {translation} \\n True Translation: {true_translation} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Translate a sample of input sentences from test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: i could never give up meat \n",
      " Predicted Translation:  ich haette nie aufgehoert dass sie selten haben _END \n",
      " True Translation: START_ ich koennte niemals ohne fleisch auskommen _END \n",
      "\n",
      "Source: he often plays guitar \n",
      " Predicted Translation:  er spielt manchmal an tom _END \n",
      " True Translation: START_ er spielt oft gitarre _END \n",
      "\n",
      "Source: tom and mary didnt have a choice \n",
      " Predicted Translation:  tom und maria haben keine kinder _END \n",
      " True Translation: START_ tom und maria hatten keine wahl _END \n",
      "\n",
      "Source: the water is nice and cool \n",
      " Predicted Translation:  das wasser ist sehr heiss _END \n",
      " True Translation: START_ das wasser ist angenehm kuehl _END \n",
      "\n",
      "Source: which browser are you using \n",
      " Predicted Translation:  welche kandidatin macht dich _END \n",
      " True Translation: START_ welchen netzgucker gebrauchst du _END \n",
      "\n",
      "Source: do you have one that is a little smaller \n",
      " Predicted Translation:  hast du das ziemlich eine menge auch nicht _END \n",
      " True Translation: START_ hast du eins welches kleiner ist _END \n",
      "\n",
      "Source: i forgot to close the door \n",
      " Predicted Translation:  ich habe die tuer offen nicht aufhalten _END \n",
      " True Translation: START_ ich vergass die tuer zu schliessen _END \n",
      "\n",
      "Source: im divorced \n",
      " Predicted Translation:  ich bin dick _END \n",
      " True Translation: START_ ich bin geschieden _END \n",
      "\n",
      "Source: clean up the kitchen \n",
      " Predicted Translation:  bring die badewanne nicht _END \n",
      " True Translation: START_ mach die kueche sauber _END \n",
      "\n",
      "Source: tom likes writing \n",
      " Predicted Translation:  tom schreibt gerne _END \n",
      " True Translation: START_ tom schreibt gerne _END \n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "\n",
    "# Pick first N sentences from test data\n",
    "sentences = list(X_test.iloc[:N].values)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    seq = sentence_to_seq(sentence)\n",
    "    translation = decode_sequence(seq)\n",
    "    true_translation = y_test.iloc[i]\n",
    "    print(f'Source: {sentence} \\n Predicted Translation: {translation} \\n True Translation: {true_translation} \\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a30d934768a106a0bdaa0b54b1b0ce58ac936216b32c4f047caaf50b54e34c32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
