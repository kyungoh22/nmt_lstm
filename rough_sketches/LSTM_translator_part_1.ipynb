{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda, Embedding, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string \n",
    "import regex as re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import io\n",
    "import spacy\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# df_en_de = pd.read_table('/content/gdrive/MyDrive/deu-eng/deu.txt', names=['eng', 'deu', 'attr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_de = pd.read_table('deu-eng/deu.txt', names=['eng', 'deu', 'attr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_de = df_en_de.drop('attr',axis = 1).rename(columns = {'eng':'english', 'deu':'german'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>german</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Geh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hallo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251715</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Wenn jemand Fremdes dir sagt, dass du dich wie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251716</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Wenn jemand, der nicht weiß, woher man kommt, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251717</th>\n",
       "      <td>It may be impossible to get a completely error...</td>\n",
       "      <td>Es ist wohl unmöglich, einen vollkommen fehler...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251718</th>\n",
       "      <td>I know that adding sentences only in your nati...</td>\n",
       "      <td>Ich weiß wohl, dass das ausschließliche Beitra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251719</th>\n",
       "      <td>Doubtless there exists in this world precisely...</td>\n",
       "      <td>Ohne Zweifel findet sich auf dieser Welt zu je...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251720 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  english  \\\n",
       "0                                                     Go.   \n",
       "1                                                     Hi.   \n",
       "2                                                     Hi.   \n",
       "3                                                    Run!   \n",
       "4                                                    Run.   \n",
       "...                                                   ...   \n",
       "251715  If someone who doesn't know your background sa...   \n",
       "251716  If someone who doesn't know your background sa...   \n",
       "251717  It may be impossible to get a completely error...   \n",
       "251718  I know that adding sentences only in your nati...   \n",
       "251719  Doubtless there exists in this world precisely...   \n",
       "\n",
       "                                                   german  \n",
       "0                                                    Geh.  \n",
       "1                                                  Hallo!  \n",
       "2                                              Grüß Gott!  \n",
       "3                                                   Lauf!  \n",
       "4                                                   Lauf!  \n",
       "...                                                   ...  \n",
       "251715  Wenn jemand Fremdes dir sagt, dass du dich wie...  \n",
       "251716  Wenn jemand, der nicht weiß, woher man kommt, ...  \n",
       "251717  Es ist wohl unmöglich, einen vollkommen fehler...  \n",
       "251718  Ich weiß wohl, dass das ausschließliche Beitra...  \n",
       "251719  Ohne Zweifel findet sich auf dieser Welt zu je...  \n",
       "\n",
       "[251720 rows x 2 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all characters\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: x.lower())\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove quotes\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "\n",
    "# Set of all special characters\n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "# Remove all the special characters\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "df_en_de['german']=df_en_de['german'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "# Add start and end tokens to target sequences\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x : 'START_ '+ x + ' _END')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>german</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go</td>\n",
       "      <td>START_ geh _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hi</td>\n",
       "      <td>START_ hallo _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hi</td>\n",
       "      <td>START_ grüß gott _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>run</td>\n",
       "      <td>START_ lauf _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>run</td>\n",
       "      <td>START_ lauf _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english                 german\n",
       "0      go        START_ geh _END\n",
       "1      hi      START_ hallo _END\n",
       "2      hi  START_ grüß gott _END\n",
       "3     run       START_ lauf _END\n",
       "4     run       START_ lauf _END"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en_de.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename dataframe for convenience\n",
    "pairs = df_en_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>german</th>\n",
       "      <th>english_length</th>\n",
       "      <th>german_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go</td>\n",
       "      <td>START_ geh _END</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hi</td>\n",
       "      <td>START_ hallo _END</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hi</td>\n",
       "      <td>START_ grüß gott _END</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>run</td>\n",
       "      <td>START_ lauf _END</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>run</td>\n",
       "      <td>START_ lauf _END</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251567</th>\n",
       "      <td>charles moore created forth in an attempt to i...</td>\n",
       "      <td>START_ charles moore entwickelte forth in eine...</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251568</th>\n",
       "      <td>every student who has graduated from our unive...</td>\n",
       "      <td>START_ jeder student mit einem abschluss von u...</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251570</th>\n",
       "      <td>its dangerous to assume that all of the senten...</td>\n",
       "      <td>START_ es ist eine gefährliche annahme dass al...</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251587</th>\n",
       "      <td>the color green is often associated with money...</td>\n",
       "      <td>START_ die farbe grün wird oft mit geld und de...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251632</th>\n",
       "      <td>a great relationship is based on two main prin...</td>\n",
       "      <td>START_ eine ausgezeichnete beziehung fußt auf ...</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251049 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  english  \\\n",
       "0                                                      go   \n",
       "1                                                      hi   \n",
       "2                                                      hi   \n",
       "3                                                     run   \n",
       "4                                                     run   \n",
       "...                                                   ...   \n",
       "251567  charles moore created forth in an attempt to i...   \n",
       "251568  every student who has graduated from our unive...   \n",
       "251570  its dangerous to assume that all of the senten...   \n",
       "251587  the color green is often associated with money...   \n",
       "251632  a great relationship is based on two main prin...   \n",
       "\n",
       "                                                   german  english_length  \\\n",
       "0                                         START_ geh _END               1   \n",
       "1                                       START_ hallo _END               1   \n",
       "2                                   START_ grüß gott _END               1   \n",
       "3                                        START_ lauf _END               1   \n",
       "4                                        START_ lauf _END               1   \n",
       "...                                                   ...             ...   \n",
       "251567  START_ charles moore entwickelte forth in eine...              15   \n",
       "251568  START_ jeder student mit einem abschluss von u...              20   \n",
       "251570  START_ es ist eine gefährliche annahme dass al...              20   \n",
       "251587  START_ die farbe grün wird oft mit geld und de...              20   \n",
       "251632  START_ eine ausgezeichnete beziehung fußt auf ...              18   \n",
       "\n",
       "        german_length  \n",
       "0                   3  \n",
       "1                   3  \n",
       "2                   4  \n",
       "3                   3  \n",
       "4                   3  \n",
       "...               ...  \n",
       "251567             19  \n",
       "251568             18  \n",
       "251570             18  \n",
       "251587             20  \n",
       "251632             20  \n",
       "\n",
       "[251049 rows x 4 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs['english_length'] = pairs['english'].apply(lambda x: len(x.split()))\n",
    "pairs['german_length'] = pairs['german'].apply(lambda x: len(x.split()))\n",
    "max_len = 20\n",
    "pairs = pairs[pairs['english_length'] <= max_len]\n",
    "pairs = pairs[pairs['german_length'] <= max_len]\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary of English\n",
    "all_en_words=set()\n",
    "for eng in pairs['english']:\n",
    "    for word in eng.split():\n",
    "        if word not in all_en_words:\n",
    "            all_en_words.add(word)\n",
    "\n",
    "# Vocabulary of German \n",
    "all_de_words=set()\n",
    "for de in pairs['german']:\n",
    "    for word in de.split():\n",
    "        if word not in all_de_words:\n",
    "            all_de_words.add(word)\n",
    "\n",
    "# Max Length of source sequence\n",
    "length_list=[]\n",
    "for l in pairs['english']:\n",
    "    length_list.append(len(l.split(' ')))\n",
    "max_length_src = np.max(length_list)\n",
    "\n",
    "# Max Length of target sequence\n",
    "length_list=[]\n",
    "for l in pairs['german']:\n",
    "    length_list.append(len(l.split(' ')))\n",
    "max_length_tar = np.max(length_list)\n",
    "\n",
    "input_words = sorted(list(all_en_words))\n",
    "target_words = sorted(list(all_de_words))\n",
    "\n",
    "# Calculate Vocab size for both source and target\n",
    "num_encoder_tokens = len(all_en_words)\n",
    "num_decoder_tokens = len(all_de_words)\n",
    "\n",
    "#\"\"\" find out why you add 1\"\"\"\n",
    "#num_decoder_tokens += 1 # For zero padding \n",
    "\n",
    "# Create word to token dictionary for both source and target\n",
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "\n",
    "# Create token to word dictionary for both source and target\n",
    "reverse_input_token_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_token_index = dict((i, word) for word, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(367, 368)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_index['START_'], target_token_index['_END']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20, 37773)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_src, max_length_tar, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                        go\n",
       "1                                                        hi\n",
       "2                                                        hi\n",
       "3                                                       run\n",
       "4                                                       run\n",
       "                                ...                        \n",
       "251567    charles moore created forth in an attempt to i...\n",
       "251568    every student who has graduated from our unive...\n",
       "251570    its dangerous to assume that all of the senten...\n",
       "251587    the color green is often associated with money...\n",
       "251632    a great relationship is based on two main prin...\n",
       "Name: english, Length: 251049, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs['english']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with a small dataset first, due to problems with dealing with too large a batch at a time\n",
    "X_train, X_test, y_train, y_test = train_test_split(pairs['english'], pairs['german'], test_size=0.99, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['he is my classmate', 'hes been asleep for ten hours now',\n",
       "       'i dont know whos involved', ...,\n",
       "       'we must go there whether we like it or not',\n",
       "       'who would look after my children if i died',\n",
       "       'i should probably lose a few pounds'], dtype=object)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2510,)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):          # j = batch number\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            \n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
    "            \n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            decoder_target_data = np.transpose(decoder_target_data, axes = [1, 0, 2])\n",
    "            decoder_target_data = list(decoder_target_data)\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = generate_batch()\n",
    "[encoder_input_data, decoder_input_data], decoder_target_data = next(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(encoder_input_data)\n",
    "type(decoder_input_data)\n",
    "type(decoder_target_data)\n",
    "decoder_target_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 37773)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Model for training </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = max_len\n",
    "Ty = Tx\n",
    "\n",
    "# Modify these later based on spacy's word vectors\n",
    "input_embedding_dims = 100\n",
    "output_embedding_dims = 100\n",
    "\n",
    "n_a = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers as global variables\n",
    "\n",
    "# Create layer objects LSTM_cell and densor\n",
    "decoder_LSTM_cell = LSTM (n_a, return_state = True)        \n",
    "densor = Dense(num_decoder_tokens, activation='softmax')\n",
    "\n",
    "# Create reshaper object – will be used in function \"learning_model\"\n",
    "reshaper = Reshape((1, output_embedding_dims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Training Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, None])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa = Input(shape = (None,))\n",
    "aaa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoder part ###\n",
    "\n",
    "# Input layer for encoder (English)\n",
    "encoder_inputs = Input(shape=(Tx,), dtype = 'int32')            # (None, Tx) -- sequence of integers\n",
    "\n",
    "# Embedding layer for encoder (English)\n",
    "enc_emb_layer = Embedding (input_dim = num_encoder_tokens + 1, output_dim = input_embedding_dims, mask_zero = True)\n",
    "\n",
    "# Turn input sequence (English) into embedding vectors\n",
    "encoder_embeddings = enc_emb_layer(encoder_inputs)                 # (None, Tx, input_embedding_dims)\n",
    "\n",
    "# LSTM layer for encoder (English)\n",
    "encoder_lstm = LSTM(units = n_a, return_state = True, name = 'encoder_LSTM')           # Note that this LSTM layer computes on ALL Tx values at once.\n",
    "\n",
    "# Pass input embedding vectors (English) through encoder LSTM\n",
    "encoder_outputs, a, c = encoder_lstm(encoder_embeddings)\n",
    "\n",
    "# Save state vectors from encoder LSTM\n",
    "encoder_states = [a,c]\n",
    "\n",
    "\n",
    "### Decoder part ###\n",
    "\n",
    "# Input layer for decoder (German)\n",
    "decoder_inputs = Input(shape = (Ty,), dtype = 'int32')      # (None, Ty)\n",
    "\n",
    "# Embedding layer for decoder (German)\n",
    "dec_emb_layer = Embedding(input_dim = num_decoder_tokens + 1, output_dim = output_embedding_dims, mask_zero = True)\n",
    "\n",
    "# Turn decoder input sequence (German) into embedding vectors\n",
    "decoder_embeddings = dec_emb_layer(decoder_inputs)  # (None, Ty, output_embedding_dims)\n",
    "\n",
    "# Initialise list of outputs\n",
    "decoder_outputs = []\n",
    "\n",
    "# Loop over each time-step of decoder input (German) \n",
    "for t in range (Ty):\n",
    "    # Select embedding vector for time-step t\n",
    "    dec_emb_t = decoder_embeddings[:,t,:]       # (None, output_embedding_dims)\n",
    "\n",
    "    # Reshape embedding vector for time-step t       \n",
    "    dec_emb_t = reshaper(dec_emb_t)             # (None, 1, output_embedding_dims)\n",
    "\n",
    "    # Get a and c for time-step t from decoder LSTM \n",
    "    a, _, c = decoder_LSTM_cell(inputs = dec_emb_t, initial_state = encoder_states)\n",
    "    encoder_states = [a,c]\n",
    "\n",
    "    # Pass a for time-step t from decoder LSTM through Dense layer\n",
    "    out = densor(a)\n",
    "    decoder_outputs.append(out)\n",
    "\n",
    "# encoder_inputs is numpy array\n",
    "# decoder_inputs is numpy array\n",
    "# decoder_outputs is list of numpy arrays (one-hot vectors)\n",
    "model = Model (inputs = [encoder_inputs, decoder_inputs], outputs = decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Inference Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get a and c vectors from encoder ####\n",
    "\n",
    "# Create embedding vectors for encoder input sequence (English)\n",
    "                                                                     # encoder_inputs = (None, Tx)\n",
    "encoder_embeddings_2 = enc_emb_layer(encoder_inputs)                 # (None, Tx, input_embedding_dims)\n",
    "\n",
    "# Pass embedding vectors through encoder LSTM\n",
    "encoder_outputs_2, a_2, c_2 = encoder_lstm(encoder_embeddings_2)     # Use same encoding LSTM layer\n",
    "\n",
    "# Store a and c vectors from encoder\n",
    "encoder_states_2 = [a_2,c_2]                                         \n",
    "\n",
    "# Create model. \n",
    "# inputs = encoder input sequence; outputs = a and c vectors\n",
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states_2)          #encoder_inputs = (None, Tx)\n",
    "                                                                                    # encoder_states_2 = [a_2, c_2], a_2 & c_2 are numpy arrays        \n",
    "\n",
    "\n",
    "### Use a and c vectors from encoder to generate prediction ###\n",
    "\n",
    "# Input layers for a0 and c0 vectors into decoder\n",
    "#decoder_input2 = Input(shape = (None,))\n",
    "decoder_state_input_a = Input(shape = (n_a))                # (None, n_a)\n",
    "decoder_state_input_c = Input(shape = (n_a))                # (None, n_a)\n",
    "decoder_states_inputs = [decoder_state_input_a, decoder_state_input_c]\n",
    "\n",
    "# Convert decoder input sequence (German) into embedding vectors\n",
    "                                                                # decoder_inputs = (None, Ty)\n",
    "decoder_embeddings_2 = dec_emb_layer(decoder_inputs)            # (None, Ty, output_embedding_dims)\n",
    "\n",
    "# Select embedding vector for time-step 0 (START_ token)\n",
    "decoder_embeddings_2 = decoder_embeddings_2[:,0,:]              # (None, output_embedding_dims)\n",
    "# Reshape into correct dimensions\n",
    "decoder_embeddings_2 = reshaper(decoder_embeddings_2)           # (None, 1, output_embedding_dims)\n",
    "\n",
    "outputs2 = []\n",
    "\n",
    "# look over all time-steps in the decoder input sequence\n",
    "# number of time_steps will increase by 1 each time we make a prediction later\n",
    "for t in range(decoder_inputs.shape[1]):                 \n",
    "\n",
    "    # pass in a, c, and decoder input vectors (time-step t-1) into decoder LSTM\n",
    "    # Note, inputs (for decoder_LSTM_cell) = (None, 1, output_embedding_dims)\n",
    "    # Get updated a, c vectors\n",
    "    decoder_state_input_a, _, decoder_state_input_c = decoder_LSTM_cell(inputs = decoder_embeddings_2, initial_state = decoder_states_inputs)\n",
    "    decoder_state_inputs = [decoder_state_input_a, decoder_state_input_c]\n",
    "\n",
    "    # Pass \"a\" vector through Dense layer\n",
    "    out = densor(decoder_state_input_a)                  # out = (None, num_decoder_tokens) \n",
    "\n",
    "    \n",
    "    # Find index for prediction\n",
    "    max_idx = tf.math.argmax(out, -1)                       # max_idx = (None, 1)\n",
    "    # Convert prediction into embedding vector\n",
    "    decoder_embeddings_2 = dec_emb_layer(max_idx+1)           # decoder_embeddings_2 = (None, output_embedding_dims)\n",
    "\n",
    "    # Reshape embedding vector\n",
    "    decoder_embeddings_2 = reshaper(decoder_embeddings_2)   # (None, 1, output_embedding_dims)\n",
    "\n",
    "# decoder_inputs = numpy array\n",
    "# decoder_states_inputs = list of numpy arrays\n",
    "# [decoder_inputs] + decoder+states_inputs = list of numpy arrays\n",
    "decoder_model = Model(inputs = [decoder_inputs]+ decoder_states_inputs, \n",
    "                    outputs = out                           # \"out\" = softmax output for final time-step inspected\n",
    "                                                            # out = (None, num_decoder_tokens)\n",
    "                    )                                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 10, 11]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = [1,2,3]\n",
    "list2 = [10, 11]\n",
    "list1 + list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar1 = np.zeros((1,1))\n",
    "ar1 = np.append(ar1, 1)\n",
    "ar1 = np.append(ar1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar1.reshape(1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'befüllte'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_target_token_index[3999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    \"\"\"\n",
    "    input_seq = numpy array of sequences (1,Ty)\n",
    "    \"\"\"\n",
    "\n",
    "    # Encode the input as state vectors.\n",
    "    states_values = encoder_model.predict(input_seq)         # input_seq = (1,Ty)\n",
    "                                                             # states_values = [a, c] from encoder, where a,c are numpy arrays\n",
    "    \n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,Ty))                            \n",
    "    \n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']          # One integer\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    j = 1\n",
    "    while not stop_condition and j <Ty:\n",
    "        print(target_seq.shape, target_seq)\n",
    "        output_tokens = decoder_model.predict([target_seq] + states_values)     \n",
    "        # inputs = [decoder_inputs]+ decoder_states_inputs = list of three numpy arrays\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[:, 1])\n",
    "        sampled_char = reverse_target_token_index[sampled_token_index + 1]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "        print(decoded_sentence, len(decoded_sentence))\n",
    "        \n",
    "        # Exit condition: either hit max length or find stop token.\n",
    "        if (sampled_char == '_END' or len(decoded_sentence.split()) > Ty):\n",
    "            stop_condition = True\n",
    "        \n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq[0,j] = sampled_token_index+1\n",
    "        \n",
    "        j+=1\n",
    "        print(j)\n",
    "        \n",
    "    \n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 39s 618ms/step - loss: 58.5120 - dense_8_loss: 7.3553 - dense_8_1_loss: 8.4977 - dense_8_2_loss: 7.9895 - dense_8_3_loss: 8.3832 - dense_8_4_loss: 7.5750 - dense_8_5_loss: 6.0467 - dense_8_6_loss: 4.3263 - dense_8_7_loss: 2.9849 - dense_8_8_loss: 1.9643 - dense_8_9_loss: 1.2915 - dense_8_10_loss: 0.8003 - dense_8_11_loss: 0.4746 - dense_8_12_loss: 0.2945 - dense_8_13_loss: 0.2207 - dense_8_14_loss: 0.1595 - dense_8_15_loss: 0.0720 - dense_8_16_loss: 0.0466 - dense_8_17_loss: 0.0213 - dense_8_18_loss: 0.0083 - dense_8_19_loss: 0.0000e+00 - dense_8_accuracy: 0.0082 - dense_8_1_accuracy: 0.0012 - dense_8_2_accuracy: 0.0082 - dense_8_3_accuracy: 0.0600 - dense_8_4_accuracy: 0.1501 - dense_8_5_accuracy: 0.1933 - dense_8_6_accuracy: 0.1678 - dense_8_7_accuracy: 0.1209 - dense_8_8_accuracy: 0.0847 - dense_8_9_accuracy: 0.0580 - dense_8_10_accuracy: 0.0432 - dense_8_11_accuracy: 0.0230 - dense_8_12_accuracy: 0.0123 - dense_8_13_accuracy: 0.0070 - dense_8_14_accuracy: 0.0082 - dense_8_15_accuracy: 0.0041 - dense_8_16_accuracy: 0.0029 - dense_8_17_accuracy: 0.0016 - dense_8_18_accuracy: 8.2237e-04 - dense_8_19_accuracy: 0.0000e+00                                      \n"
     ]
    }
   ],
   "source": [
    "# encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states_2)\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "#(encoder_input_data, decoder_input_data), decoder_target_data = generate_batch()\n",
    "history = model.fit(generate_batch(), steps_per_epoch=X_train.shape[0]//128, epochs = epochs, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpt_sq = X_train[0].split()\n",
    "inpt_sq = [input_token_index[word] for word in inpt_sq]\n",
    "#inpt_sq = np.array(inpt_sq).reshape(1,5)\n",
    "#inpt_sq.shape\n",
    "\n",
    "max_len = 20\n",
    "input_data = np.zeros((1,max_len))\n",
    "for i, num in enumerate(inpt_sq):\n",
    "  input_data[0,i] = num\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20) [[367.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.]]\n",
      " 0 2\n",
      "2\n",
      "(1, 20) [[367.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.]]\n",
      " 0 0 4\n",
      "3\n",
      "(1, 20) [[367.   1.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.]]\n",
      " 0 0 0 6\n",
      "4\n",
      "(1, 20) [[367.   1.   1.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.]]\n",
      " 0 0 0 0 8\n",
      "5\n",
      "(1, 20) [[367.   1.   1.   1.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.]]\n",
      " 0 0 0 0 0 10\n",
      "6\n",
      "(1, 20) [[367.   1.   1.   1.   1.   1.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.]]\n",
      " 0 0 0 0 0 0 12\n",
      "7\n",
      "(1, 20) [[367.   1.   1.   1.   1.   1.   1.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.]]\n",
      " 0 0 0 0 0 0 0 14\n",
      "8\n",
      "(1, 20) [[367.   1.   1.   1.   1.   1.   1.   1.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.]]\n",
      " 0 0 0 0 0 0 0 0 16\n",
      "9\n",
      "(1, 20) [[367.   1.   1.   1.   1.   1.   1.   1.   1.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.]]\n",
      " 0 0 0 0 0 0 0 0 0 18\n",
      "10\n",
      "(1, 20) [[367.   1.   1.   1.   1.   1.   1.   1.   1.   1.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.]]\n",
      " 0 0 0 0 0 0 0 0 0 0 20\n",
      "11\n",
      "(1, 20) [[367.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.]]\n",
      " 0 0 0 0 0 0 0 0 0 0 0 22\n",
      "12\n",
      "(1, 20) [[367.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.]]\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 24\n",
      "13\n",
      "(1, 20) [[367.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   0.\n",
      "    0.   0.   0.   0.   0.   0.]]\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 26\n",
      "14\n",
      "(1, 20) [[367.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "    0.   0.   0.   0.   0.   0.]]\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 28\n",
      "15\n",
      "(1, 20) [[367.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   0.   0.   0.   0.   0.]]\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 30\n",
      "16\n",
      "(1, 20) [[367.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   1.   0.   0.   0.   0.]]\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 32\n",
      "17\n",
      "(1, 20) [[367.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   1.   1.   0.   0.   0.]]\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 34\n",
      "18\n",
      "(1, 20) [[367.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   0.   0.]]\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 36\n",
      "19\n",
      "(1, 20) [[367.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   0.]]\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 38\n",
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sequence(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('deep_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62f0de500e91648e2f1c8ecd59ca95f97588cc062e27f09a44618e0428f97b74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
