{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/deep_learning/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda, Embedding, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string \n",
    "import regex as re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import io\n",
    "import spacy\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# df_en_de = pd.read_table('/content/gdrive/MyDrive/deu-eng/deu.txt', names=['eng', 'deu', 'attr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_de = pd.read_table('deu-eng/deu.txt', names=['eng', 'deu', 'attr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_de = df_en_de.drop('attr',axis = 1).rename(columns = {'eng':'english', 'deu':'german'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>german</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Geh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hallo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251715</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Wenn jemand Fremdes dir sagt, dass du dich wie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251716</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Wenn jemand, der nicht weiß, woher man kommt, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251717</th>\n",
       "      <td>It may be impossible to get a completely error...</td>\n",
       "      <td>Es ist wohl unmöglich, einen vollkommen fehler...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251718</th>\n",
       "      <td>I know that adding sentences only in your nati...</td>\n",
       "      <td>Ich weiß wohl, dass das ausschließliche Beitra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251719</th>\n",
       "      <td>Doubtless there exists in this world precisely...</td>\n",
       "      <td>Ohne Zweifel findet sich auf dieser Welt zu je...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251720 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  english  \\\n",
       "0                                                     Go.   \n",
       "1                                                     Hi.   \n",
       "2                                                     Hi.   \n",
       "3                                                    Run!   \n",
       "4                                                    Run.   \n",
       "...                                                   ...   \n",
       "251715  If someone who doesn't know your background sa...   \n",
       "251716  If someone who doesn't know your background sa...   \n",
       "251717  It may be impossible to get a completely error...   \n",
       "251718  I know that adding sentences only in your nati...   \n",
       "251719  Doubtless there exists in this world precisely...   \n",
       "\n",
       "                                                   german  \n",
       "0                                                    Geh.  \n",
       "1                                                  Hallo!  \n",
       "2                                              Grüß Gott!  \n",
       "3                                                   Lauf!  \n",
       "4                                                   Lauf!  \n",
       "...                                                   ...  \n",
       "251715  Wenn jemand Fremdes dir sagt, dass du dich wie...  \n",
       "251716  Wenn jemand, der nicht weiß, woher man kommt, ...  \n",
       "251717  Es ist wohl unmöglich, einen vollkommen fehler...  \n",
       "251718  Ich weiß wohl, dass das ausschließliche Beitra...  \n",
       "251719  Ohne Zweifel findet sich auf dieser Welt zu je...  \n",
       "\n",
       "[251720 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all characters\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: x.lower())\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove quotes\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "\n",
    "# Set of all special characters\n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "# Remove all the special characters\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "df_en_de['german']=df_en_de['german'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "# Add start and end tokens to target sequences\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x : 'START_ '+ x + ' _END')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>german</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go</td>\n",
       "      <td>START_ geh _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hi</td>\n",
       "      <td>START_ hallo _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hi</td>\n",
       "      <td>START_ grüß gott _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>run</td>\n",
       "      <td>START_ lauf _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>run</td>\n",
       "      <td>START_ lauf _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english                 german\n",
       "0      go        START_ geh _END\n",
       "1      hi      START_ hallo _END\n",
       "2      hi  START_ grüß gott _END\n",
       "3     run       START_ lauf _END\n",
       "4     run       START_ lauf _END"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en_de.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename dataframe for convenience\n",
    "pairs = df_en_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209317\n",
      "2093\n"
     ]
    }
   ],
   "source": [
    "pairs = df_en_de\n",
    "pairs['english_length'] = pairs['english'].apply(lambda x: len(x.split(' ')))\n",
    "pairs['german_length'] = pairs['german'].apply(lambda x: len(x.split(' ')))\n",
    "max_len = 10\n",
    "pairs = pairs[pairs['english_length'] <= max_len]\n",
    "pairs = pairs[pairs['german_length'] <= max_len]\n",
    "print (len(pairs))\n",
    "pairs = pairs.sample(frac = 0.01)\n",
    "print(len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary of English\n",
    "all_en_words=set()\n",
    "for eng in pairs['english']:\n",
    "    for word in eng.split():\n",
    "        if word not in all_en_words:\n",
    "            all_en_words.add(word)\n",
    "\n",
    "# Vocabulary of German \n",
    "all_de_words=set()\n",
    "for de in pairs['german']:\n",
    "    for word in de.split():\n",
    "        if word not in all_de_words:\n",
    "            all_de_words.add(word)\n",
    "\n",
    "# Max Length of source sequence\n",
    "length_list=[]\n",
    "for l in pairs['english']:\n",
    "    length_list.append(len(l.split(' ')))\n",
    "max_length_src = np.max(length_list)\n",
    "\n",
    "# Max Length of target sequence\n",
    "length_list=[]\n",
    "for l in pairs['german']:\n",
    "    length_list.append(len(l.split(' ')))\n",
    "max_length_tar = np.max(length_list)\n",
    "\n",
    "\n",
    "input_words = sorted(list(all_en_words))\n",
    "target_words = sorted(list(all_de_words))\n",
    "\n",
    "# Calculate Vocab size for both source and target\n",
    "num_encoder_tokens = len(all_en_words) + 1\n",
    "num_decoder_tokens = len(all_de_words) + 1\n",
    "\n",
    "#\"\"\" find out why you add 1\"\"\"\n",
    "#num_decoder_tokens += 1 # For zero padding \n",
    "\n",
    "# Create word to token dictionary for both source and target\n",
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "\n",
    "# Create token to word dictionary for both source and target\n",
    "reverse_input_token_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_token_index = dict((i, word) for word, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 18)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_index['START_'], target_token_index['_END']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 2082, 2659)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_src, max_length_tar, num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4004                                  i felt naked\n",
       "86533                     tom got up and went away\n",
       "74469                       its hard to please tom\n",
       "69958                       you arrived a bit late\n",
       "100436                  he put the key in the lock\n",
       "                            ...                   \n",
       "181154           you both love each other dont you\n",
       "206124    tom poured some apple juice into a glass\n",
       "134011               tom nervously opened the door\n",
       "9284                                 ill go see it\n",
       "94423                    she is a very poor driver\n",
       "Name: english, Length: 2093, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs['english']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = pairs['english'], pairs['german']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1883,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):          # j = batch number\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            \n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
    "            \n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1. \n",
    "                        \"\"\" This should be target_token_index[word] - 1\"\"\"\n",
    "            # decoder_target_data = np.transpose(decoder_target_data, axes = [1, 0, 2])\n",
    "            # decoder_target_data = list(decoder_target_data)\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Model for training </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tx = max_len\n",
    "# Ty = Tx\n",
    "\n",
    "# # Modify these later based on spacy's word vectors\n",
    "# input_embedding_dims = 100\n",
    "# output_embedding_dims = 100\n",
    "\n",
    "# n_a = 64\n",
    "\n",
    "train_samples = len(X_train)\n",
    "val_samples = len (X_test)\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "latent_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Input layers, we define \"time-step\" number as None\n",
    "# As such: we can input variables of different time-step lengths\n",
    "# This will be useful during the prediction stage, where we will feed one word at a time\n",
    "\n",
    "# All layer objects are global variables. \n",
    "# Their weights are remembered when we call on them in a later model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,))                                                       # (None, None) -- (m, Tx)\n",
    "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)      # (None, None, latent_dim) \n",
    "                                                                                            # -- (m, Tx, embedding dimensions)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)                                          \n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)                                   # encoder_outputs = (None, latent_dim)\n",
    "                                                                                            # -- (m, state vector dimensions)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]                                                         # state_h = (None, 256)\n",
    "                                                                                            # -- (m, state vector dimensions)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))                                                       # (None, None) -- (m, Ty)\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)                 \n",
    "dec_emb = dec_emb_layer(decoder_inputs)                                                     # (None, None, latent_dim) -- (m, Ty, embedding dimensions)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)                   \n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
    "                                     initial_state=encoder_states)                          # (None, None, latent_dim) -- (m, Ty, state vector dimensions)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')                             \n",
    "decoder_outputs = decoder_dense(decoder_outputs)                                            # (None, None, num_decoder_tokens)\n",
    "                                                                                            # (m, Ty, decoder vocab size)\n",
    "\n",
    "# Define the model that takes encoder and decoder input \n",
    "# to output decoder_outputs\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)                            # encoder_inputs = (None, None) -- (m, Tx)\n",
    "                                                                                            # decoder_inputs = (None, None) -- (m, Ty)\n",
    "                                                                                            # decoder_outputs = (None, None, decoder vocab size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= 'rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = len(X_train) # Total Training samples\n",
    "val_samples = len(X_test)    # Total validation or test samples\n",
    "batch_size = 128\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1883,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14/14 [==============================] - 8s 147ms/step - loss: 4.2039 - acc: 0.1636\n",
      "Epoch 2/100\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 3.4809 - acc: 0.1898\n",
      "Epoch 3/100\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 3.3764 - acc: 0.1916\n",
      "Epoch 4/100\n",
      "14/14 [==============================] - 2s 146ms/step - loss: 3.3390 - acc: 0.1966\n",
      "Epoch 5/100\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 3.2642 - acc: 0.2017\n",
      "Epoch 6/100\n",
      "14/14 [==============================] - 2s 145ms/step - loss: 3.1986 - acc: 0.2091\n",
      "Epoch 7/100\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 3.1628 - acc: 0.2137\n",
      "Epoch 8/100\n",
      "14/14 [==============================] - 2s 145ms/step - loss: 3.1114 - acc: 0.2205\n",
      "Epoch 9/100\n",
      "14/14 [==============================] - 2s 156ms/step - loss: 3.0586 - acc: 0.2303\n",
      "Epoch 10/100\n",
      "14/14 [==============================] - 2s 149ms/step - loss: 2.9635 - acc: 0.2541\n",
      "Epoch 11/100\n",
      "14/14 [==============================] - 2s 150ms/step - loss: 2.9209 - acc: 0.2592\n",
      "Epoch 12/100\n",
      "14/14 [==============================] - 2s 145ms/step - loss: 2.8558 - acc: 0.2654\n",
      "Epoch 13/100\n",
      "14/14 [==============================] - 2s 149ms/step - loss: 2.8058 - acc: 0.2693\n",
      "Epoch 14/100\n",
      "14/14 [==============================] - 2s 146ms/step - loss: 2.7427 - acc: 0.2739\n",
      "Epoch 15/100\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 2.6881 - acc: 0.2804\n",
      "Epoch 16/100\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 2.6898 - acc: 0.2851\n",
      "Epoch 17/100\n",
      "14/14 [==============================] - 2s 145ms/step - loss: 2.5723 - acc: 0.2888\n",
      "Epoch 18/100\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 2.5247 - acc: 0.2965\n",
      "Epoch 19/100\n",
      "14/14 [==============================] - 2s 146ms/step - loss: 2.4966 - acc: 0.2993\n",
      "Epoch 20/100\n",
      "14/14 [==============================] - 2s 146ms/step - loss: 2.4277 - acc: 0.3042\n",
      "Epoch 21/100\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 2.3839 - acc: 0.3112\n",
      "Epoch 22/100\n",
      "14/14 [==============================] - 2s 146ms/step - loss: 2.3567 - acc: 0.3138\n",
      "Epoch 23/100\n",
      "14/14 [==============================] - 2s 150ms/step - loss: 2.3125 - acc: 0.3206\n",
      "Epoch 24/100\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 2.2746 - acc: 0.3217\n",
      "Epoch 25/100\n",
      "14/14 [==============================] - 2s 146ms/step - loss: 2.2181 - acc: 0.3280\n",
      "Epoch 26/100\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 2.1931 - acc: 0.3300\n",
      "Epoch 27/100\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 2.1516 - acc: 0.3351\n",
      "Epoch 28/100\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 2.1107 - acc: 0.3394\n",
      "Epoch 29/100\n",
      "14/14 [==============================] - 2s 145ms/step - loss: 2.0709 - acc: 0.3432\n",
      "Epoch 30/100\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 2.0306 - acc: 0.3507\n",
      "Epoch 31/100\n",
      "14/14 [==============================] - 2s 149ms/step - loss: 2.0293 - acc: 0.3596\n",
      "Epoch 32/100\n",
      "14/14 [==============================] - 2s 145ms/step - loss: 1.9378 - acc: 0.3690\n",
      "Epoch 33/100\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 1.9121 - acc: 0.3691\n",
      "Epoch 34/100\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 1.8776 - acc: 0.3834\n",
      "Epoch 35/100\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 1.8249 - acc: 0.3903\n",
      "Epoch 36/100\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 1.7927 - acc: 0.4007\n",
      "Epoch 37/100\n",
      "14/14 [==============================] - 2s 146ms/step - loss: 1.7572 - acc: 0.4090\n",
      "Epoch 38/100\n",
      "14/14 [==============================] - 2s 150ms/step - loss: 1.7240 - acc: 0.4201\n",
      "Epoch 39/100\n",
      "14/14 [==============================] - 2s 146ms/step - loss: 1.6869 - acc: 0.4293\n",
      "Epoch 40/100\n",
      "14/14 [==============================] - 2s 149ms/step - loss: 1.6402 - acc: 0.4419\n",
      "Epoch 41/100\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 1.6124 - acc: 0.4509\n",
      "Epoch 42/100\n",
      "14/14 [==============================] - 2s 146ms/step - loss: 1.5757 - acc: 0.4602\n",
      "Epoch 43/100\n",
      "14/14 [==============================] - 2s 149ms/step - loss: 1.5352 - acc: 0.4776\n",
      "Epoch 44/100\n",
      "14/14 [==============================] - 2s 144ms/step - loss: 1.5022 - acc: 0.4854\n",
      "Epoch 45/100\n",
      "14/14 [==============================] - 2s 149ms/step - loss: 1.4651 - acc: 0.5026\n",
      "Epoch 46/100\n",
      "14/14 [==============================] - 2s 144ms/step - loss: 1.4524 - acc: 0.5160\n",
      "Epoch 47/100\n",
      "14/14 [==============================] - 2s 149ms/step - loss: 1.3827 - acc: 0.5324\n",
      "Epoch 48/100\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 1.3457 - acc: 0.5493\n",
      "Epoch 49/100\n",
      "14/14 [==============================] - 2s 146ms/step - loss: 1.3246 - acc: 0.5583\n",
      "Epoch 50/100\n",
      "14/14 [==============================] - 2s 144ms/step - loss: 1.2803 - acc: 0.5717\n",
      "Epoch 51/100\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 1.2461 - acc: 0.5863\n",
      "Epoch 52/100\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 1.2095 - acc: 0.5999\n",
      "Epoch 53/100\n",
      "14/14 [==============================] - 2s 150ms/step - loss: 1.1810 - acc: 0.6106\n",
      "Epoch 54/100\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 1.1486 - acc: 0.6259\n",
      "Epoch 55/100\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 1.1035 - acc: 0.6417\n",
      "Epoch 56/100\n",
      "14/14 [==============================] - 2s 146ms/step - loss: 1.0791 - acc: 0.6507\n",
      "Epoch 57/100\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 1.0503 - acc: 0.6592\n",
      "Epoch 58/100\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 1.0137 - acc: 0.6771\n",
      "Epoch 59/100\n",
      "14/14 [==============================] - 2s 150ms/step - loss: 0.9802 - acc: 0.6886\n",
      "Epoch 60/100\n",
      "14/14 [==============================] - 2s 146ms/step - loss: 0.9545 - acc: 0.7011\n",
      "Epoch 61/100\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 0.9307 - acc: 0.7109\n",
      "Epoch 62/100\n",
      "14/14 [==============================] - 2s 146ms/step - loss: 0.8779 - acc: 0.7282\n",
      "Epoch 63/100\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 0.8590 - acc: 0.7309\n",
      "Epoch 64/100\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 0.8302 - acc: 0.7464\n",
      "Epoch 65/100\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 0.7893 - acc: 0.7618\n",
      "Epoch 66/100\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 0.7669 - acc: 0.7718\n",
      "Epoch 67/100\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 0.7392 - acc: 0.7817\n",
      "Epoch 68/100\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 0.7135 - acc: 0.7926\n",
      "Epoch 69/100\n",
      "14/14 [==============================] - 2s 145ms/step - loss: 0.6833 - acc: 0.8051\n",
      "Epoch 70/100\n",
      "14/14 [==============================] - 2s 146ms/step - loss: 0.6576 - acc: 0.8138\n",
      "Epoch 71/100\n",
      "14/14 [==============================] - 2s 145ms/step - loss: 0.6308 - acc: 0.8246\n",
      "Epoch 72/100\n",
      "14/14 [==============================] - 2s 145ms/step - loss: 0.6115 - acc: 0.8282\n",
      "Epoch 73/100\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 0.5788 - acc: 0.8473\n",
      "Epoch 74/100\n",
      "14/14 [==============================] - 2s 144ms/step - loss: 0.5588 - acc: 0.8489\n",
      "Epoch 75/100\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 0.5364 - acc: 0.8605\n",
      "Epoch 76/100\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 0.5134 - acc: 0.8727\n",
      "Epoch 77/100\n",
      "14/14 [==============================] - 2s 146ms/step - loss: 0.4837 - acc: 0.8801\n",
      "Epoch 78/100\n",
      "14/14 [==============================] - 2s 149ms/step - loss: 0.4635 - acc: 0.8847\n",
      "Epoch 79/100\n",
      "14/14 [==============================] - 2s 145ms/step - loss: 0.4459 - acc: 0.8961\n",
      "Epoch 80/100\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 0.4218 - acc: 0.9018\n",
      "Epoch 81/100\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 0.4011 - acc: 0.9091\n",
      "Epoch 82/100\n",
      "14/14 [==============================] - 2s 145ms/step - loss: 0.3810 - acc: 0.9169\n",
      "Epoch 83/100\n",
      "14/14 [==============================] - 2s 152ms/step - loss: 0.3648 - acc: 0.9231\n",
      "Epoch 84/100\n",
      "14/14 [==============================] - 2s 151ms/step - loss: 0.3467 - acc: 0.9280\n",
      "Epoch 85/100\n",
      "14/14 [==============================] - 2s 153ms/step - loss: 0.3273 - acc: 0.9353\n",
      "Epoch 86/100\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 0.3088 - acc: 0.9402\n",
      "Epoch 87/100\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 0.2917 - acc: 0.9469\n",
      "Epoch 88/100\n",
      "14/14 [==============================] - 2s 149ms/step - loss: 0.2779 - acc: 0.9491\n",
      "Epoch 89/100\n",
      "14/14 [==============================] - 2s 146ms/step - loss: 0.2630 - acc: 0.9545\n",
      "Epoch 90/100\n",
      "14/14 [==============================] - 2s 146ms/step - loss: 0.2444 - acc: 0.9597\n",
      "Epoch 91/100\n",
      "14/14 [==============================] - 2s 148ms/step - loss: 0.2349 - acc: 0.9637\n",
      "Epoch 92/100\n",
      "14/14 [==============================] - 2s 152ms/step - loss: 0.2168 - acc: 0.9659\n",
      "Epoch 93/100\n",
      "14/14 [==============================] - 2s 149ms/step - loss: 0.2032 - acc: 0.9697\n",
      "Epoch 94/100\n",
      "14/14 [==============================] - 2s 156ms/step - loss: 0.1930 - acc: 0.9734\n",
      "Epoch 95/100\n",
      "14/14 [==============================] - 2s 151ms/step - loss: 0.1810 - acc: 0.9755\n",
      "Epoch 96/100\n",
      "14/14 [==============================] - 2s 151ms/step - loss: 0.1667 - acc: 0.9787\n",
      "Epoch 97/100\n",
      "14/14 [==============================] - 2s 153ms/step - loss: 0.1552 - acc: 0.9824\n",
      "Epoch 98/100\n",
      "14/14 [==============================] - 2s 145ms/step - loss: 0.1498 - acc: 0.9817\n",
      "Epoch 99/100\n",
      "14/14 [==============================] - 2s 150ms/step - loss: 0.1363 - acc: 0.9850\n",
      "Epoch 100/100\n",
      "14/14 [==============================] - 2s 146ms/step - loss: 0.1262 - acc: 0.9879\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(generate_batch(), steps_per_epoch=train_samples//batch_size, epochs = epochs, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"Context vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)                   # encoder_inputs = (None, None) -- (m, Tx)\n",
    "                                                                        # encoder_states = [state_h, state_c]\n",
    "                                                                        # [(None, latent_dim), (None,latent_dim)] \n",
    "                                                                        # -- [(m, state vector dims), (m, state vector dims)]\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step           \n",
    "decoder_state_input_h = Input(shape=(latent_dim,))                      # (None, latent_dim) -- (m, state vector dims)   \n",
    "decoder_state_input_c = Input(shape=(latent_dim,))                      # (None, latent_dim) -- (m, state vector dims)\n",
    "decoder_state_input = [decoder_state_input_h, decoder_state_input_c]\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)                                # (None, None, latent_dim) -- (m, Ty, embedding dims)\n",
    "# To predict the next word in the sequence, set the initial states \n",
    "# to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(\n",
    "                                            dec_emb2, \n",
    "                                            initial_state=decoder_state_input\n",
    "                                            )                           # decoder_outputs2 = (None, None, latent_dim) -- (m, Ty, state vector dims)\n",
    "                                                                        # state_h2 = (None, latent_dim) -- (m, state vector dims)\n",
    "                                                                        # state_c2 = (None, latent_dim) -- (m, state vector dims)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)                      # (None, None, num_decoder_tokens) -- (m, Ty, target vocab size + 1)\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_state_input,\n",
    "    [decoder_outputs2] + decoder_states2)                               # decoder_inputs = (None, None) -- (m, Ty)\n",
    "                                                                        # decoder_state_input = [(m, state vector dims), (m, state vector dims)]\n",
    "                                                                        # decoder_outputs2 = (m, Ty, target vocab size)\n",
    "                                                                        # decoder_states2 = [(m, state vector dims), (m, state vector dims)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, None, 2659])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_seq(sentence):\n",
    "    \"\"\"\n",
    "    sentence = string\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "    encoder_input_data = np.zeros((1, max_length_src))\n",
    "    \n",
    "    sentence = sentence.lower().split()\n",
    "    #print(sentence)\n",
    "    for j, word in enumerate(sentence):\n",
    "        encoder_input_data[0,j] = input_token_index[word]\n",
    "    return encoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "\n",
    "    \"\"\" \n",
    "    input_seq = (None, None) -- (m, Tx) \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)                     # states_value = [state_h, state_c]\n",
    "                                                                        # [(None, latent_dim), (None,latent_dim)] \n",
    "                                                                        # -- [(m, state vector dims), (m, state vector dims)]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of \n",
    "    #target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    # Note: target_seq will always be a single integer\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)        \n",
    "                                                                                        # target_seq = decoder_inputs = (None, None) = (m, Ty)\n",
    "                                                                                        # states_value = decoder_state_input \n",
    "                                                                                        # = [(m, state vector dims), (m, state vector dims)]\n",
    "                                                                                        \n",
    "                                                                                        # output_tokens = decoder_outputs2 = (m, Ty, target vocab dims)\n",
    "                                                                                        # h = state_h2 = (m, state vector dims)\n",
    "                                                                                        # c = state_c2 = (m, state vector dims)\n",
    "# Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word =reverse_target_token_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+ sampled_word\n",
    "# Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_word == '_END' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "# Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "# Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('we need to know what happened', ' wir müssen wissen was passiert ist _END')\n",
      "('tom promised it would never happen again', ' tom versprach dass es nie wieder vorkäme _END')\n",
      "('tom knows how to milk a cow', ' tom weiß wie man eine kuh melkt _END')\n",
      "('can we fix this', ' können wir das beheben _END')\n",
      "('is this your letter', ' ist dies dein brief _END')\n",
      "('my assistant will be able to handle that', ' mein assistent wird damit schon klarkommen _END')\n",
      "('tom isnt new here', ' tom ist nicht neu hier _END')\n",
      "('tom is wearing someone elses coat', ' tom trägt jemandes anderen mantel _END')\n",
      "('tom is living in the past', ' tom lebt in der vergangenheit _END')\n",
      "('tom was untidy', ' tom war unordentlich _END')\n"
     ]
    }
   ],
   "source": [
    "sentences = list(X_train.iloc[:10].values)\n",
    "\n",
    "translations = []\n",
    "for sentence in sentences:\n",
    "    seq = sentence_to_seq(sentence)\n",
    "    translation = decode_sequence(seq)\n",
    "    translations.append(translation)\n",
    "\n",
    "sentence_translation_pairs = zip (sentences, translations)\n",
    "for elem in sentence_translation_pairs:\n",
    "    print (elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('deep_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62f0de500e91648e2f1c8ecd59ca95f97588cc062e27f09a44618e0428f97b74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
