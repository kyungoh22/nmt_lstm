{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSTM translator without pre-trained embeddings\n",
    "- This model works, so keep it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/deep_learning/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda, Embedding, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string \n",
    "import regex as re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import io\n",
    "import spacy\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# df_en_de = pd.read_table('/content/gdrive/MyDrive/deu-eng/deu.txt', names=['eng', 'deu', 'attr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_de = pd.read_table('deu-eng/deu.txt', names=['eng', 'deu', 'attr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_de = df_en_de.drop('attr',axis = 1).rename(columns = {'eng':'english', 'deu':'german'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>german</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Geh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hallo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251715</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Wenn jemand Fremdes dir sagt, dass du dich wie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251716</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Wenn jemand, der nicht weiß, woher man kommt, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251717</th>\n",
       "      <td>It may be impossible to get a completely error...</td>\n",
       "      <td>Es ist wohl unmöglich, einen vollkommen fehler...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251718</th>\n",
       "      <td>I know that adding sentences only in your nati...</td>\n",
       "      <td>Ich weiß wohl, dass das ausschließliche Beitra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251719</th>\n",
       "      <td>Doubtless there exists in this world precisely...</td>\n",
       "      <td>Ohne Zweifel findet sich auf dieser Welt zu je...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251720 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  english  \\\n",
       "0                                                     Go.   \n",
       "1                                                     Hi.   \n",
       "2                                                     Hi.   \n",
       "3                                                    Run!   \n",
       "4                                                    Run.   \n",
       "...                                                   ...   \n",
       "251715  If someone who doesn't know your background sa...   \n",
       "251716  If someone who doesn't know your background sa...   \n",
       "251717  It may be impossible to get a completely error...   \n",
       "251718  I know that adding sentences only in your nati...   \n",
       "251719  Doubtless there exists in this world precisely...   \n",
       "\n",
       "                                                   german  \n",
       "0                                                    Geh.  \n",
       "1                                                  Hallo!  \n",
       "2                                              Grüß Gott!  \n",
       "3                                                   Lauf!  \n",
       "4                                                   Lauf!  \n",
       "...                                                   ...  \n",
       "251715  Wenn jemand Fremdes dir sagt, dass du dich wie...  \n",
       "251716  Wenn jemand, der nicht weiß, woher man kommt, ...  \n",
       "251717  Es ist wohl unmöglich, einen vollkommen fehler...  \n",
       "251718  Ich weiß wohl, dass das ausschließliche Beitra...  \n",
       "251719  Ohne Zweifel findet sich auf dieser Welt zu je...  \n",
       "\n",
       "[251720 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all characters\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: x.lower())\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove quotes\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "\n",
    "# Set of all special characters\n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "# Remove all the special characters\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "df_en_de['german']=df_en_de['german'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "# Add start and end tokens to target sequences\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x : 'START_ '+ x + ' _END')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>german</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go</td>\n",
       "      <td>START_ geh _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hi</td>\n",
       "      <td>START_ hallo _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hi</td>\n",
       "      <td>START_ grüß gott _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>run</td>\n",
       "      <td>START_ lauf _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>run</td>\n",
       "      <td>START_ lauf _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english                 german\n",
       "0      go        START_ geh _END\n",
       "1      hi      START_ hallo _END\n",
       "2      hi  START_ grüß gott _END\n",
       "3     run       START_ lauf _END\n",
       "4     run       START_ lauf _END"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en_de.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename dataframe for convenience\n",
    "pairs = df_en_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209317\n"
     ]
    }
   ],
   "source": [
    "max_len = 10\n",
    "\n",
    "pairs = df_en_de\n",
    "pairs['english_length'] = pairs['english'].apply(lambda x: len(x.split(' ')))\n",
    "pairs['german_length'] = pairs['german'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "pairs = pairs[pairs['english_length'] <= max_len]\n",
    "pairs = pairs[pairs['german_length'] <= max_len]\n",
    "print(len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary of English\n",
    "all_en_words=set()\n",
    "for eng in pairs['english']:\n",
    "    for word in eng.split():\n",
    "        if word not in all_en_words:\n",
    "            all_en_words.add(word)\n",
    "\n",
    "# Vocabulary of German \n",
    "all_de_words=set()\n",
    "for de in pairs['german']:\n",
    "    for word in de.split():\n",
    "        if word not in all_de_words:\n",
    "            all_de_words.add(word)\n",
    "\n",
    "# Max Length of source sequence\n",
    "length_list=[]\n",
    "for l in pairs['english']:\n",
    "    length_list.append(len(l.split(' ')))\n",
    "max_length_src = np.max(length_list)\n",
    "\n",
    "# Max Length of target sequence\n",
    "length_list=[]\n",
    "for l in pairs['german']:\n",
    "    length_list.append(len(l.split(' ')))\n",
    "max_length_tar = np.max(length_list)\n",
    "\n",
    "\n",
    "input_words = sorted(list(all_en_words))\n",
    "target_words = sorted(list(all_de_words))\n",
    "\n",
    "# Calculate Vocab size for both source and target\n",
    "# Add 1 for zero padding\n",
    "num_encoder_tokens = len(all_en_words) + 1\n",
    "num_decoder_tokens = len(all_de_words) + 1\n",
    "\n",
    "\n",
    "\n",
    "# Create word to token dictionary for both source and target\n",
    "#input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "#target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "input_word_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_word_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "\n",
    "# Create token to word dictionary for both source and target\n",
    "# reverse_input_token_index = dict((i, word) for word, i in input_token_index.items())\n",
    "# reverse_target_token_index = dict((i, word) for word, i in target_token_index.items())\n",
    "input_index_word = dict((i, word) for word, i in input_word_index.items())\n",
    "target_index_word = dict((i, word) for word, i in target_word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(286, 287)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_word_index['START_'], target_word_index['_END']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 15171, 31410)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_src, max_length_tar, num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                        go\n",
       "1                                                        hi\n",
       "2                                                        hi\n",
       "3                                                       run\n",
       "4                                                       run\n",
       "                                ...                        \n",
       "247790    the police couldnt find any footprints outside...\n",
       "247935    ethnic minorities struggle against prejudice p...\n",
       "248043    the united nations general assembly adopted th...\n",
       "248243    magicians trick their audience into believing ...\n",
       "248398    civilization is the limitless multiplication o...\n",
       "Name: english, Length: 209317, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs['english']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = pairs['english'], pairs['german']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167453,)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):          # j = batch number\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            \n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
    "            \n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_word_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_word_index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_word_index[word]] = 1. \n",
    "                        \"\"\" This should be target_token_index[word] - 1\"\"\"\n",
    "            # decoder_target_data = np.transpose(decoder_target_data, axes = [1, 0, 2])\n",
    "            # decoder_target_data = list(decoder_target_data)\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Model for training </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Input layers, we define \"time-step\" number as None\n",
    "# As such: we can input variables of different time-step lengths\n",
    "# This will be useful during the prediction stage, where we will feed one word at a time\n",
    "\n",
    "# All layer objects are global variables. \n",
    "# Their weights are remembered when we call on them in a later model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,))                                                       # (None, None) -- (m, Tx)\n",
    "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)      # (None, None, latent_dim) \n",
    "                                                                                            # -- (m, Tx, embedding dimensions)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)                                          \n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)                                   # encoder_outputs = (None, latent_dim)\n",
    "                                                                                            # -- (m, state vector dimensions)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]                                                         # state_h = (None, 256)\n",
    "                                                                                            # -- (m, state vector dimensions)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))                                                       # (None, None) -- (m, Ty)\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)                 \n",
    "dec_emb = dec_emb_layer(decoder_inputs)                                                     # (None, None, latent_dim) -- (m, Ty, embedding dimensions)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)                   \n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
    "                                     initial_state=encoder_states)                          # (None, None, latent_dim) -- (m, Ty, state vector dimensions)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')                             \n",
    "decoder_outputs = decoder_dense(decoder_outputs)                                            # (None, None, num_decoder_tokens)\n",
    "                                                                                            # (m, Ty, decoder vocab size)\n",
    "\n",
    "# Define the model that takes encoder and decoder input \n",
    "# to output decoder_outputs\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)                            # encoder_inputs = (None, None) -- (m, Tx)\n",
    "                                                                                            # decoder_inputs = (None, None) -- (m, Ty)\n",
    "                                                                                            # decoder_outputs = (None, None, decoder vocab size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= 'Adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = len(X_train) # Total Training samples\n",
    "val_samples = len(X_test) # total validation samples\n",
    "batch_size = 128\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167453,)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20/1308 [..............................] - ETA: 17:10 - loss: 6.3545 - acc: 0.1595"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/LSTM_translator/LSTM_translator_part_4.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/LSTM_translator/LSTM_translator_part_4.ipynb#ch0000023?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(generate_batch(), \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/LSTM_translator/LSTM_translator_part_4.ipynb#ch0000023?line=1'>2</a>\u001b[0m                     steps_per_epoch\u001b[39m=\u001b[39;49mtrain_samples\u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49mbatch_size, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/LSTM_translator/LSTM_translator_part_4.ipynb#ch0000023?line=2'>3</a>\u001b[0m                     epochs \u001b[39m=\u001b[39;49m epochs, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/LSTM_translator/LSTM_translator_part_4.ipynb#ch0000023?line=3'>4</a>\u001b[0m                     validation_data \u001b[39m=\u001b[39;49m generate_batch(X_test, y_test, batch_size \u001b[39m=\u001b[39;49m batch_size), \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/LSTM_translator/LSTM_translator_part_4.ipynb#ch0000023?line=4'>5</a>\u001b[0m                     validation_steps \u001b[39m=\u001b[39;49m val_samples \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m batch_size, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/LSTM_translator/LSTM_translator_part_4.ipynb#ch0000023?line=5'>6</a>\u001b[0m                     verbose \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deep_learning/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deep_learning/lib/python3.9/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1385\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deep_learning/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deep_learning/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deep_learning/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deep_learning/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2957\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deep_learning/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1854\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deep_learning/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deep_learning/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(generate_batch(), \n",
    "                    steps_per_epoch=train_samples//batch_size, \n",
    "                    epochs = epochs, \n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size), \n",
    "                    validation_steps = val_samples // batch_size, \n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"Context vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)                   # encoder_inputs = (None, None) -- (m, Tx)\n",
    "                                                                        # encoder_states = [state_h, state_c]\n",
    "                                                                        # [(None, latent_dim), (None,latent_dim)] \n",
    "                                                                        # -- [(m, state vector dims), (m, state vector dims)]\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step           \n",
    "decoder_state_input_h = Input(shape=(latent_dim,))                      # (None, latent_dim) -- (m, state vector dims)   \n",
    "decoder_state_input_c = Input(shape=(latent_dim,))                      # (None, latent_dim) -- (m, state vector dims)\n",
    "decoder_state_input = [decoder_state_input_h, decoder_state_input_c]\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)                                # (None, None, latent_dim) -- (m, Ty, embedding dims)\n",
    "# To predict the next word in the sequence, set the initial states \n",
    "# to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(\n",
    "                                            dec_emb2, \n",
    "                                            initial_state=decoder_state_input\n",
    "                                            )                           # decoder_outputs2 = (None, None, latent_dim) -- (m, Ty, state vector dims)\n",
    "                                                                        # state_h2 = (None, latent_dim) -- (m, state vector dims)\n",
    "                                                                        # state_c2 = (None, latent_dim) -- (m, state vector dims)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)                      # (None, None, num_decoder_tokens) -- (m, Ty, target vocab size + 1)\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_state_input,\n",
    "    [decoder_outputs2] + decoder_states2)                               # decoder_inputs = (None, None) -- (m, Ty)\n",
    "                                                                        # decoder_state_input = [(m, state vector dims), (m, state vector dims)]\n",
    "                                                                        # decoder_outputs2 = (m, Ty, target vocab size)\n",
    "                                                                        # decoder_states2 = [(m, state vector dims), (m, state vector dims)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_seq(sentence):\n",
    "    \"\"\"\n",
    "    sentence = string\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "    encoder_input_data = np.zeros((1, max_length_src))\n",
    "    \n",
    "    sentence = sentence.lower().split()\n",
    "    #print(sentence)\n",
    "    for j, word in enumerate(sentence):\n",
    "        encoder_input_data[0,j] = input_word_index[word]\n",
    "    return encoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "\n",
    "    \"\"\" \n",
    "    input_seq = (None, None) -- (m, Tx) \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)                     # states_value = [state_h, state_c]\n",
    "                                                                        # [(None, latent_dim), (None,latent_dim)] \n",
    "                                                                        # -- [(m, state vector dims), (m, state vector dims)]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of \n",
    "    #target sequence with the start character.\n",
    "    target_seq[0, 0] = target_word_index['START_']\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    # Note: target_seq will always be a single integer\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)        \n",
    "                                                                                        # target_seq = decoder_inputs = (None, None) = (m, Ty)\n",
    "                                                                                        # states_value = decoder_state_input \n",
    "                                                                                        # = [(m, state vector dims), (m, state vector dims)]\n",
    "                                                                                        \n",
    "                                                                                        # output_tokens = decoder_outputs2 = (m, Ty, target vocab dims)\n",
    "                                                                                        # h = state_h2 = (m, state vector dims)\n",
    "                                                                                        # c = state_c2 = (m, state vector dims)\n",
    "# Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word =target_index_word[sampled_token_index]\n",
    "        decoded_sentence += ' '+ sampled_word\n",
    "# Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_word == '_END' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "# Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "# Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i agree to your terms', ' ich akzeptiere deine bedingungen _END')\n",
      "('tom did a foolish thing', ' tom hat eine dummheit gemacht _END')\n",
      "('tom has no idea what hes doing', ' tom hat gestern nicht zurückgeschrieben _END')\n",
      "('tom often borrowed money from his friends', ' tom lieh sich oftmals geld von seinen freunden _END')\n",
      "('do you really think that could happen', ' denkst du das wirklich mit dem leben _END')\n",
      "('tom is really talented', ' tom ist sehr begabt _END')\n",
      "('tom isnt very trustworthy is he', ' tom ist nicht sehr vertrauenswürdig nicht wahr _END')\n",
      "('if i were you id stay quiet', ' ich bliebe an deiner stelle ruhig _END')\n",
      "('i am glad to see her', ' ich bin wegen tom zu sein _END')\n",
      "('tom has just bought a new computer', ' tom hat viele freundinnen _END')\n"
     ]
    }
   ],
   "source": [
    "sentences = list(X_train.iloc[:10].values)\n",
    "\n",
    "translations = []\n",
    "for sentence in sentences:\n",
    "    seq = sentence_to_seq(sentence)\n",
    "    translation = decode_sequence(seq)\n",
    "    translations.append(translation)\n",
    "\n",
    "sentence_translation_pairs = zip (sentences, translations)\n",
    "for elem in sentence_translation_pairs:\n",
    "    print (elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('deep_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62f0de500e91648e2f1c8ecd59ca95f97588cc062e27f09a44618e0428f97b74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
